{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [],
      "mount_file_id": "1rR-A9OAQGSQDx1KGTS6p3eG_bWZoaMzr",
      "authorship_tag": "ABX9TyO0F4YejXVBHSjNysKChNTZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jingut/EX/blob/main/Exp_14.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "WTIagXU6IISm"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 1. 데이터 수집하기"
      ],
      "metadata": {
        "id": "CDRMfNNmHsgR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "csv_path = '/content/drive/MyDrive/aiffel/transformer_chatbot/data/ChatbotData.csv'\n",
        "data = pd.read_csv(csv_path)\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "ZudkzWaAIH17",
        "outputId": "d42b2751-cb54-4e05-9a4d-c77f15a06ac5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                             Q                         A  label\n",
              "0                       12시 땡!                하루가 또 가네요.      0\n",
              "1                  1지망 학교 떨어졌어                 위로해 드립니다.      0\n",
              "2                 3박4일 놀러가고 싶다               여행은 언제나 좋죠.      0\n",
              "3              3박4일 정도 놀러가고 싶다               여행은 언제나 좋죠.      0\n",
              "4                      PPL 심하네                눈살이 찌푸려지죠.      0\n",
              "...                        ...                       ...    ...\n",
              "11818           훔쳐보는 것도 눈치 보임.        티가 나니까 눈치가 보이는 거죠!      2\n",
              "11819           훔쳐보는 것도 눈치 보임.             훔쳐보는 거 티나나봐요.      2\n",
              "11820              흑기사 해주는 짝남.                    설렜겠어요.      2\n",
              "11821  힘든 연애 좋은 연애라는게 무슨 차이일까?  잘 헤어질 수 있는 사이 여부인 거 같아요.      2\n",
              "11822               힘들어서 결혼할까봐        도피성 결혼은 하지 않길 바라요.      2\n",
              "\n",
              "[11823 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-465128c8-c43f-4a62-aafa-0b693e91e331\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Q</th>\n",
              "      <th>A</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>12시 땡!</td>\n",
              "      <td>하루가 또 가네요.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1지망 학교 떨어졌어</td>\n",
              "      <td>위로해 드립니다.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3박4일 놀러가고 싶다</td>\n",
              "      <td>여행은 언제나 좋죠.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3박4일 정도 놀러가고 싶다</td>\n",
              "      <td>여행은 언제나 좋죠.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>PPL 심하네</td>\n",
              "      <td>눈살이 찌푸려지죠.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11818</th>\n",
              "      <td>훔쳐보는 것도 눈치 보임.</td>\n",
              "      <td>티가 나니까 눈치가 보이는 거죠!</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11819</th>\n",
              "      <td>훔쳐보는 것도 눈치 보임.</td>\n",
              "      <td>훔쳐보는 거 티나나봐요.</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11820</th>\n",
              "      <td>흑기사 해주는 짝남.</td>\n",
              "      <td>설렜겠어요.</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11821</th>\n",
              "      <td>힘든 연애 좋은 연애라는게 무슨 차이일까?</td>\n",
              "      <td>잘 헤어질 수 있는 사이 여부인 거 같아요.</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11822</th>\n",
              "      <td>힘들어서 결혼할까봐</td>\n",
              "      <td>도피성 결혼은 하지 않길 바라요.</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11823 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-465128c8-c43f-4a62-aafa-0b693e91e331')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-465128c8-c43f-4a62-aafa-0b693e91e331 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-465128c8-c43f-4a62-aafa-0b693e91e331');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 사용할 샘플의 최대 개수\n",
        "MAX_SAMPLES = len(data)\n",
        "print(MAX_SAMPLES)"
      ],
      "metadata": {
        "id": "17-EEA_qIaWx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f94e68b5-d4e7-4489-9311-985fed169b04"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11823\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 사용할 샘플의 최대 개수\n",
        "MAX_SAMPLES = len(data)\n",
        "print(MAX_SAMPLES)"
      ],
      "metadata": {
        "id": "mFQqNbUxIcUi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd8c9a48-82cc-4b45-b65f-2ae8a610932b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11823\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 답변(A)\n",
        "data['A'].values[0]"
      ],
      "metadata": {
        "id": "3iaYlphNIcQp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "db5581af-00f7-4ea0-8cb4-cbd841470160"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'하루가 또 가네요.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 답변(A)\n",
        "data['A'].values[0]"
      ],
      "metadata": {
        "id": "OpJl9owCIcOl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "951e8b49-2f62-4677-cbe8-cfe5c4ea1862"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'하루가 또 가네요.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 답변(A)\n",
        "data['A'].values[0]"
      ],
      "metadata": {
        "id": "p6b5bs0RIfz-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "8f018004-a78a-4c97-afff-f1f1411830ab"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'하루가 또 가네요.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 2. 데이터 전처리하기"
      ],
      "metadata": {
        "id": "i2V2NhJGHway"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 전처리 함수\n",
        "def preprocess_sentence(sentence):\n",
        "  sentence = sentence.lower().strip()\n",
        "\n",
        "  # 단어와 구두점(punctuation) 사이의 거리를 만듭니다.\n",
        "  # 예를 들어서 \"I am a student.\" => \"I am a student .\"와 같이\n",
        "  # student와 온점 사이에 거리를 만듭니다.\n",
        "  sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
        "  sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
        "\n",
        "  # (가-힣, \".\", \"?\", \"!\", \",\")를 제외한 모든 문자를 공백인 ' '로 대체합니다.\n",
        "  sentence = re.sub(r\"[^가-힣?.!,]+\", \" \", sentence)\n",
        "  sentence = sentence.strip()\n",
        "  return sentence"
      ],
      "metadata": {
        "id": "IwgLDawBIfx4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 질문과 답변의 쌍인 데이터셋을 구성하기 위한 데이터 로드 함수\n",
        "def load_conversations():\n",
        "  inputs, outputs = [], []\n",
        "  for i in range(MAX_SAMPLES):\n",
        "    # 전처리 함수를 질문에 해당되는 inputs와 답변에 해당되는 outputs에 적용.\n",
        "    inputs.append(preprocess_sentence(data['Q'].values[i]))\n",
        "    outputs.append(preprocess_sentence(data['A'].values[i]))\n",
        "\n",
        "  return inputs, outputs"
      ],
      "metadata": {
        "id": "ZiH55kbuIfvS"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions, answers = load_conversations()\n",
        "print('전체 샘플 수 :', len(questions))\n",
        "print('전체 샘플 수 :', len(answers))"
      ],
      "metadata": {
        "id": "R5QsbWQEItlM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf631129-0fae-409c-be16-a9ad2a666bca"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "전체 샘플 수 : 11823\n",
            "전체 샘플 수 : 11823\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "questions, answers = load_conversations()\n",
        "print('전체 샘플 수 :', len(questions))\n",
        "print('전체 샘플 수 :', len(answers))"
      ],
      "metadata": {
        "id": "57DgZiQDIxiT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6fdcdc9-dc32-477b-8021-ad40776a117e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "전체 샘플 수 : 11823\n",
            "전체 샘플 수 : 11823\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 3. SubwordTextEncoder 사용하기"
      ],
      "metadata": {
        "id": "IbX674uJHy-2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_datasets as tfds\n",
        "print(\"살짝 오래 걸릴 수 있어요. 스트레칭 한 번 해볼까요? 👐\")\n",
        "\n",
        "# 질문과 답변 데이터셋에 대해서 Vocabulary 생성. (Tensorflow 2.3.0 이상) (클라우드는 2.4 입니다)\n",
        "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(questions + answers, target_vocab_size=2**13)"
      ],
      "metadata": {
        "id": "25zpKv2GI0_H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e071399f-2b7d-40aa-c039-29abd8c1f5f7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "살짝 오래 걸릴 수 있어요. 스트레칭 한 번 해볼까요? 👐\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 시작 토큰과 종료 토큰에 고유한 정수를 부여합니다.\n",
        "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]"
      ],
      "metadata": {
        "id": "3YAkZWjyI48P"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('START_TOKEN의 번호 :' ,[tokenizer.vocab_size])\n",
        "print('END_TOKEN의 번호 :' ,[tokenizer.vocab_size + 1])"
      ],
      "metadata": {
        "id": "_Cb96eH1I9Ky",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c521990-fc21-45d6-b8cf-202ad50cd83b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "START_TOKEN의 번호 : [8127]\n",
            "END_TOKEN의 번호 : [8128]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 시작 토큰과 종료 토큰을 고려하여 +2를 하여 단어장의 크기를 산정합니다.\n",
        "VOCAB_SIZE = tokenizer.vocab_size + 2\n",
        "print(VOCAB_SIZE)"
      ],
      "metadata": {
        "id": "_t-coWEzI-y9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a13c546-a1d5-4bf5-fe3d-de8875d9f3cb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8129\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 임의의 22번째 샘플에 대해서 정수 인코딩 작업을 수행.\n",
        "print('정수 인코딩 후의 21번째 질문 샘플: {}'.format(tokenizer.encode(questions[21])))\n",
        "print('정수 인코딩 후의 21번째 답변 샘플: {}'.format(tokenizer.encode(answers[21])))"
      ],
      "metadata": {
        "id": "d8ul0C7EJAiJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "901b2a36-9645-4248-b274-fd43300262b0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "정수 인코딩 후의 21번째 질문 샘플: [5742, 612, 2481, 4148]\n",
            "정수 인코딩 후의 21번째 답변 샘플: [2352, 7481, 7, 6245, 97, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 샘플의 최대 허용 길이 또는 패딩 후의 최종 길이\n",
        "MAX_LENGTH = 40\n",
        "print(MAX_LENGTH)"
      ],
      "metadata": {
        "id": "WTbi0N_4JCIO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cebc61b1-7391-4f9a-ba6c-8bc2cd1b5518"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 정수 인코딩, 최대 길이를 초과하는 샘플 제거, 패딩\n",
        "def tokenize_and_filter(inputs, outputs):\n",
        "  tokenized_inputs, tokenized_outputs = [], []\n",
        "  \n",
        "  for (sentence1, sentence2) in zip(inputs, outputs):\n",
        "    # 정수 인코딩 과정에서 시작 토큰과 종료 토큰을 추가\n",
        "    sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
        "    sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
        "\n",
        "    # 최대 길이 40 이하인 경우에만 데이터셋으로 허용\n",
        "    if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n",
        "      tokenized_inputs.append(sentence1)\n",
        "      tokenized_outputs.append(sentence2)\n",
        "  \n",
        "  # 최대 길이 40으로 모든 데이터셋을 패딩\n",
        "  tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "      tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
        "  tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "      tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
        "  \n",
        "  return tokenized_inputs, tokenized_outputs"
      ],
      "metadata": {
        "id": "Dv3PzrpLJDRI"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions, answers = tokenize_and_filter(questions, answers)\n",
        "print('단어장의 크기 :',(VOCAB_SIZE))\n",
        "print('필터링 후의 질문 샘플 개수: {}'.format(len(questions)))\n",
        "print('필터링 후의 답변 샘플 개수: {}'.format(len(answers)))"
      ],
      "metadata": {
        "id": "zu76HjH9JGUY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a44ba14-8c95-4133-cf68-278bf3877220"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어장의 크기 : 8129\n",
            "필터링 후의 질문 샘플 개수: 11823\n",
            "필터링 후의 답변 샘플 개수: 11823\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##교사강요(Teaching Force)"
      ],
      "metadata": {
        "id": "T4ie9SHgJKi2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 20000\n",
        "\n",
        "# 디코더는 이전의 target을 다음의 input으로 사용합니다.\n",
        "# 이에 따라 outputs에서는 START_TOKEN을 제거하겠습니다.\n",
        "dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    {\n",
        "        'inputs': questions,\n",
        "        'dec_inputs': answers[:, :-1]\n",
        "    },\n",
        "    {\n",
        "        'outputs': answers[:, 1:]\n",
        "    },\n",
        "))\n",
        "\n",
        "dataset = dataset.cache()\n",
        "dataset = dataset.shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE)\n",
        "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "metadata": {
        "id": "07nJJk2YJIV7"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 4. 모델 구성하기"
      ],
      "metadata": {
        "id": "olNSGhwrH1qc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 포지셔널 인코딩 레이어\n",
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, position, d_model):\n",
        "    super(PositionalEncoding, self).__init__()\n",
        "    self.pos_encoding = self.positional_encoding(position, d_model)\n",
        "\n",
        "  def get_angles(self, position, i, d_model):\n",
        "    angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
        "    return position * angles\n",
        "\n",
        "  def positional_encoding(self, position, d_model):\n",
        "    # 각도 배열 생성\n",
        "    angle_rads = self.get_angles(\n",
        "        position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
        "        i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
        "        d_model=d_model)\n",
        "\n",
        "    # 배열의 짝수 인덱스에는 sin 함수 적용\n",
        "    sines = tf.math.sin(angle_rads[:, 0::2])\n",
        "    # 배열의 홀수 인덱스에는 cosine 함수 적용\n",
        "    cosines = tf.math.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    # sin과 cosine이 교차되도록 재배열\n",
        "    pos_encoding = tf.stack([sines, cosines], axis=0)\n",
        "    pos_encoding = tf.transpose(pos_encoding,[1, 2, 0]) \n",
        "    pos_encoding = tf.reshape(pos_encoding, [position, d_model])\n",
        "\n",
        "    pos_encoding = pos_encoding[tf.newaxis, ...]\n",
        "    return tf.cast(pos_encoding, tf.float32)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"
      ],
      "metadata": {
        "id": "Fo0BspB3JTTa"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 스케일드 닷 프로덕트 어텐션 함수\n",
        "def scaled_dot_product_attention(query, key, value, mask):\n",
        "  # 어텐션 가중치는 Q와 K의 닷 프로덕트\n",
        "  matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
        "\n",
        "  # 가중치를 정규화\n",
        "  depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
        "  logits = matmul_qk / tf.math.sqrt(depth)\n",
        "\n",
        "  # 패딩에 마스크 추가\n",
        "  if mask is not None:\n",
        "    logits += (mask * -1e9)\n",
        "\n",
        "  # softmax적용\n",
        "  attention_weights = tf.nn.softmax(logits, axis=-1)\n",
        "\n",
        "  # 최종 어텐션은 가중치와 V의 닷 프로덕트\n",
        "  output = tf.matmul(attention_weights, value)\n",
        "  return output"
      ],
      "metadata": {
        "id": "SP4Uri3EJidJ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
        "    super(MultiHeadAttention, self).__init__(name=name)\n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = d_model\n",
        "\n",
        "    assert d_model % self.num_heads == 0\n",
        "\n",
        "    self.depth = d_model // self.num_heads\n",
        "\n",
        "    self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
        "    self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
        "    self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
        "\n",
        "    self.dense = tf.keras.layers.Dense(units=d_model)\n",
        "\n",
        "  def split_heads(self, inputs, batch_size):\n",
        "    inputs = tf.reshape(\n",
        "        inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
        "    return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
        "\n",
        "  def call(self, inputs):\n",
        "    query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
        "        'value'], inputs['mask']\n",
        "    batch_size = tf.shape(query)[0]\n",
        "\n",
        "    # Q, K, V에 각각 Dense를 적용합니다\n",
        "    query = self.query_dense(query)\n",
        "    key = self.key_dense(key)\n",
        "    value = self.value_dense(value)\n",
        "\n",
        "    # 병렬 연산을 위한 머리를 여러 개 만듭니다\n",
        "    query = self.split_heads(query, batch_size)\n",
        "    key = self.split_heads(key, batch_size)\n",
        "    value = self.split_heads(value, batch_size)\n",
        "\n",
        "    # 스케일드 닷 프로덕트 어텐션 함수\n",
        "    scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
        "\n",
        "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "\n",
        "    # 어텐션 연산 후에 각 결과를 다시 연결(concatenate)합니다\n",
        "    concat_attention = tf.reshape(scaled_attention,\n",
        "                                  (batch_size, -1, self.d_model))\n",
        "\n",
        "    # 최종 결과에도 Dense를 한 번 더 적용합니다\n",
        "    outputs = self.dense(concat_attention)\n",
        "\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "FC-_H6BOJoIx"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##마스킹\n"
      ],
      "metadata": {
        "id": "0FxUpOVJJszt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_padding_mask(x):\n",
        "  mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
        "  # (batch_size, 1, 1, sequence length)\n",
        "  return mask[:, tf.newaxis, tf.newaxis, :]"
      ],
      "metadata": {
        "id": "j8vc8_g7JrYO"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_look_ahead_mask(x):\n",
        "  seq_len = tf.shape(x)[1]\n",
        "  look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "  padding_mask = create_padding_mask(x)\n",
        "  return tf.maximum(look_ahead_mask, padding_mask)"
      ],
      "metadata": {
        "id": "goitdB31J09j"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 인코더 하나의 레이어를 함수로 구현.\n",
        "# 이 하나의 레이어 안에는 두 개의 서브 레이어가 존재합니다.\n",
        "def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
        "  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
        "\n",
        "  # 패딩 마스크 사용\n",
        "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
        "\n",
        "  # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
        "  attention = MultiHeadAttention(\n",
        "      d_model, num_heads, name=\"attention\")({\n",
        "          'query': inputs,\n",
        "          'key': inputs,\n",
        "          'value': inputs,\n",
        "          'mask': padding_mask\n",
        "      })\n",
        "\n",
        "  # 어텐션의 결과는 Dropout과 Layer Normalization이라는 훈련을 돕는 테크닉을 수행\n",
        "  attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
        "  attention = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(inputs + attention)\n",
        "\n",
        "  # 두 번째 서브 레이어 : 2개의 완전연결층\n",
        "  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
        "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
        "\n",
        "  # 완전연결층의 결과는 Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
        "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
        "  outputs = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(attention + outputs)\n",
        "\n",
        "  return tf.keras.Model(\n",
        "      inputs=[inputs, padding_mask], outputs=outputs, name=name)"
      ],
      "metadata": {
        "id": "M7BMZoMLJ4nC"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encoder(vocab_size,\n",
        "            num_layers,\n",
        "            units,\n",
        "            d_model,\n",
        "            num_heads,\n",
        "            dropout,\n",
        "            name=\"encoder\"):\n",
        "  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
        "\n",
        "  # 패딩 마스크 사용\n",
        "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
        "\n",
        "  # 임베딩 레이어\n",
        "  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
        "  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
        "\n",
        "  # 포지셔널 인코딩\n",
        "  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
        "\n",
        "  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
        "\n",
        "  # num_layers만큼 쌓아올린 인코더의 층.\n",
        "  for i in range(num_layers):\n",
        "    outputs = encoder_layer(\n",
        "        units=units,\n",
        "        d_model=d_model,\n",
        "        num_heads=num_heads,\n",
        "        dropout=dropout,\n",
        "        name=\"encoder_layer_{}\".format(i),\n",
        "    )([outputs, padding_mask])\n",
        "\n",
        "  return tf.keras.Model(\n",
        "      inputs=[inputs, padding_mask], outputs=outputs, name=name)"
      ],
      "metadata": {
        "id": "WYIZ3vPdJ6f3"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##디코더"
      ],
      "metadata": {
        "id": "6HUjHhCDJ7xo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 디코더 하나의 레이어를 함수로 구현.\n",
        "# 이 하나의 레이어 안에는 세 개의 서브 레이어가 존재합니다.\n",
        "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
        "  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
        "  enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
        "  look_ahead_mask = tf.keras.Input(\n",
        "      shape=(1, None, None), name=\"look_ahead_mask\")\n",
        "  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
        "\n",
        "  # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
        "  attention1 = MultiHeadAttention(\n",
        "      d_model, num_heads, name=\"attention_1\")(inputs={\n",
        "          'query': inputs,\n",
        "          'key': inputs,\n",
        "          'value': inputs,\n",
        "          'mask': look_ahead_mask\n",
        "      })\n",
        "\n",
        "  # 멀티 헤드 어텐션의 결과는 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
        "  attention1 = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(attention1 + inputs)\n",
        "\n",
        "  # 두 번째 서브 레이어 : 마스크드 멀티 헤드 어텐션 수행 (인코더-디코더 어텐션)\n",
        "  attention2 = MultiHeadAttention(\n",
        "      d_model, num_heads, name=\"attention_2\")(inputs={\n",
        "          'query': attention1,\n",
        "          'key': enc_outputs,\n",
        "          'value': enc_outputs,\n",
        "          'mask': padding_mask\n",
        "      })\n",
        "\n",
        "  # 마스크드 멀티 헤드 어텐션의 결과는\n",
        "  # Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
        "  attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
        "  attention2 = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(attention2 + attention1)\n",
        "\n",
        "  # 세 번째 서브 레이어 : 2개의 완전연결층\n",
        "  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)\n",
        "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
        "\n",
        "  # 완전연결층의 결과는 Dropout과 LayerNormalization 수행\n",
        "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
        "  outputs = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(outputs + attention2)\n",
        "\n",
        "  return tf.keras.Model(\n",
        "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask], outputs=outputs, name=name)"
      ],
      "metadata": {
        "id": "GGnABNyBKFJw"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decoder(vocab_size,\n",
        "            num_layers,\n",
        "            units,\n",
        "            d_model,\n",
        "            num_heads,\n",
        "            dropout,\n",
        "            name='decoder'):\n",
        "  inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
        "  enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
        "  look_ahead_mask = tf.keras.Input(\n",
        "      shape=(1, None, None), name='look_ahead_mask')\n",
        "\n",
        "  # 패딩 마스크\n",
        "  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
        "  \n",
        "  # 임베딩 레이어\n",
        "  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
        "  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
        "\n",
        "  # 포지셔널 인코딩\n",
        "  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
        "\n",
        "  # Dropout이라는 훈련을 돕는 테크닉을 수행\n",
        "  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
        "\n",
        "  for i in range(num_layers):\n",
        "    outputs = decoder_layer(\n",
        "        units=units,\n",
        "        d_model=d_model,\n",
        "        num_heads=num_heads,\n",
        "        dropout=dropout,\n",
        "        name='decoder_layer_{}'.format(i),\n",
        "    )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
        "\n",
        "  return tf.keras.Model(\n",
        "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
        "      outputs=outputs,\n",
        "      name=name)"
      ],
      "metadata": {
        "id": "sryvzF-gJ9DZ"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##트랜스포머 함수 정의"
      ],
      "metadata": {
        "id": "GrRoSh57KT7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transformer(vocab_size,\n",
        "                num_layers,\n",
        "                units,\n",
        "                d_model,\n",
        "                num_heads,\n",
        "                dropout,\n",
        "                name=\"transformer\"):\n",
        "  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
        "  dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
        "\n",
        "  # 인코더에서 패딩을 위한 마스크\n",
        "  enc_padding_mask = tf.keras.layers.Lambda(\n",
        "      create_padding_mask, output_shape=(1, 1, None),\n",
        "      name='enc_padding_mask')(inputs)\n",
        "\n",
        "  # 디코더에서 미래의 토큰을 마스크 하기 위해서 사용합니다.\n",
        "  # 내부적으로 패딩 마스크도 포함되어져 있습니다.\n",
        "  look_ahead_mask = tf.keras.layers.Lambda(\n",
        "      create_look_ahead_mask,\n",
        "      output_shape=(1, None, None),\n",
        "      name='look_ahead_mask')(dec_inputs)\n",
        "\n",
        "  # 두 번째 어텐션 블록에서 인코더의 벡터들을 마스킹\n",
        "  # 디코더에서 패딩을 위한 마스크\n",
        "  dec_padding_mask = tf.keras.layers.Lambda(\n",
        "      create_padding_mask, output_shape=(1, 1, None),\n",
        "      name='dec_padding_mask')(inputs)\n",
        "\n",
        "  # 인코더\n",
        "  enc_outputs = encoder(\n",
        "      vocab_size=vocab_size,\n",
        "      num_layers=num_layers,\n",
        "      units=units,\n",
        "      d_model=d_model,\n",
        "      num_heads=num_heads,\n",
        "      dropout=dropout,\n",
        "  )(inputs=[inputs, enc_padding_mask])\n",
        "\n",
        "  # 디코더\n",
        "  dec_outputs = decoder(\n",
        "      vocab_size=vocab_size,\n",
        "      num_layers=num_layers,\n",
        "      units=units,\n",
        "      d_model=d_model,\n",
        "      num_heads=num_heads,\n",
        "      dropout=dropout,)(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
        "\n",
        "  # 완전연결층\n",
        "  outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
        "\n",
        "  return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)"
      ],
      "metadata": {
        "id": "Kr_gblrRKjae"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##모델생성"
      ],
      "metadata": {
        "id": "9F6jbB9-KwmM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# 하이퍼파라미터\n",
        "NUM_LAYERS = 2 # 인코더와 디코더의 층의 개수\n",
        "D_MODEL = 256 # 인코더와 디코더 내부의 입, 출력의 고정 차원\n",
        "NUM_HEADS = 8 # 멀티 헤드 어텐션에서의 헤드 수 \n",
        "UNITS = 512 # 피드 포워드 신경망의 은닉층의 크기\n",
        "DROPOUT = 0.1 # 드롭아웃의 비율\n",
        "\n",
        "model = transformer(\n",
        "    vocab_size=VOCAB_SIZE,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    units=UNITS,\n",
        "    d_model=D_MODEL,\n",
        "    num_heads=NUM_HEADS,\n",
        "    dropout=DROPOUT)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "3VoE9VnsKjYJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b0ecca2-82a6-46b4-9b2e-3e1684f926ea"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"transformer\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " inputs (InputLayer)            [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " dec_inputs (InputLayer)        [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " enc_padding_mask (Lambda)      (None, 1, 1, None)   0           ['inputs[0][0]']                 \n",
            "                                                                                                  \n",
            " encoder (Functional)           (None, None, 256)    3135232     ['inputs[0][0]',                 \n",
            "                                                                  'enc_padding_mask[0][0]']       \n",
            "                                                                                                  \n",
            " look_ahead_mask (Lambda)       (None, 1, None, Non  0           ['dec_inputs[0][0]']             \n",
            "                                e)                                                                \n",
            "                                                                                                  \n",
            " dec_padding_mask (Lambda)      (None, 1, 1, None)   0           ['inputs[0][0]']                 \n",
            "                                                                                                  \n",
            " decoder (Functional)           (None, None, 256)    3662592     ['dec_inputs[0][0]',             \n",
            "                                                                  'encoder[0][0]',                \n",
            "                                                                  'look_ahead_mask[0][0]',        \n",
            "                                                                  'dec_padding_mask[0][0]']       \n",
            "                                                                                                  \n",
            " outputs (Dense)                (None, None, 8129)   2089153     ['decoder[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 8,886,977\n",
            "Trainable params: 8,886,977\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##손실함수"
      ],
      "metadata": {
        "id": "9psqUeqCK1-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_function(y_true, y_pred):\n",
        "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
        "  \n",
        "  loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "      from_logits=True, reduction='none')(y_true, y_pred)\n",
        "\n",
        "  mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
        "  loss = tf.multiply(loss, mask)\n",
        "\n",
        "  return tf.reduce_mean(loss)"
      ],
      "metadata": {
        "id": "lJHwE_90K1cs"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##커스텀 학습률"
      ],
      "metadata": {
        "id": "Ke9226rSK5CB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super(CustomSchedule, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def __call__(self, step):\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps**-1.5)\n",
        "\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "metadata": {
        "id": "340EpbrvK5IA"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_learning_rate = CustomSchedule(d_model=128)\n",
        "\n",
        "plt.plot(sample_learning_rate(tf.range(200000, dtype=tf.float32)))\n",
        "plt.ylabel(\"Learning Rate\")\n",
        "plt.xlabel(\"Train Step\")"
      ],
      "metadata": {
        "id": "d2DwM5bUK9yK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "d9871fc5-a3a7-4ec0-8ee6-cef154213aa1"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Train Step')"
            ]
          },
          "metadata": {},
          "execution_count": 34
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = CustomSchedule(D_MODEL)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(\n",
        "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
        "\n",
        "def accuracy(y_true, y_pred):\n",
        "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
        "  return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])"
      ],
      "metadata": {
        "id": "m11QauNkLAFj"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##모델훈련"
      ],
      "metadata": {
        "id": "D737sedbLBmL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 20\n",
        "model.fit(dataset, epochs=EPOCHS, verbose=1)"
      ],
      "metadata": {
        "id": "IPUROKqLLBVg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46353754-6178-4ff6-f823-80f69595abf1"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "185/185 [==============================] - 19s 58ms/step - loss: 1.4525 - accuracy: 0.0259\n",
            "Epoch 2/20\n",
            "185/185 [==============================] - 11s 60ms/step - loss: 1.1768 - accuracy: 0.0494\n",
            "Epoch 3/20\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 1.0028 - accuracy: 0.0504\n",
            "Epoch 4/20\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.9248 - accuracy: 0.0543\n",
            "Epoch 5/20\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.8679 - accuracy: 0.0577\n",
            "Epoch 6/20\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.8079 - accuracy: 0.0620\n",
            "Epoch 7/20\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.7426 - accuracy: 0.0677\n",
            "Epoch 8/20\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.6700 - accuracy: 0.0755\n",
            "Epoch 9/20\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.5906 - accuracy: 0.0845\n",
            "Epoch 10/20\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.5085 - accuracy: 0.0936\n",
            "Epoch 11/20\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.4251 - accuracy: 0.1038\n",
            "Epoch 12/20\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.3443 - accuracy: 0.1149\n",
            "Epoch 13/20\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.2689 - accuracy: 0.1261\n",
            "Epoch 14/20\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.2045 - accuracy: 0.1362\n",
            "Epoch 15/20\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.1495 - accuracy: 0.1458\n",
            "Epoch 16/20\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.1074 - accuracy: 0.1536\n",
            "Epoch 17/20\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0782 - accuracy: 0.1589\n",
            "Epoch 18/20\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0604 - accuracy: 0.1620\n",
            "Epoch 19/20\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.0497 - accuracy: 0.1637\n",
            "Epoch 20/20\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.0455 - accuracy: 0.1642\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1f3a00d050>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 5. 모델 평가하기"
      ],
      "metadata": {
        "id": "BysD-EWaH35q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decoder_inference(sentence):\n",
        "  sentence = preprocess_sentence(sentence)\n",
        "\n",
        "  # 입력된 문장을 정수 인코딩 후, 시작 토큰과 종료 토큰을 앞뒤로 추가.\n",
        "  # ex) Where have you been? → [[8331   86   30    5 1059    7 8332]]\n",
        "  sentence = tf.expand_dims(\n",
        "      START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
        "\n",
        "  # 디코더의 현재까지의 예측한 출력 시퀀스가 지속적으로 저장되는 변수.\n",
        "  # 처음에는 예측한 내용이 없음으로 시작 토큰만 별도 저장. ex) 8331\n",
        "  output_sequence = tf.expand_dims(START_TOKEN, 0)\n",
        "\n",
        "  # 디코더의 인퍼런스 단계\n",
        "  for i in range(MAX_LENGTH):\n",
        "    # 디코더는 최대 MAX_LENGTH의 길이만큼 다음 단어 예측을 반복합니다.\n",
        "    predictions = model(inputs=[sentence, output_sequence], training=False)\n",
        "    predictions = predictions[:, -1:, :]\n",
        "\n",
        "    # 현재 예측한 단어의 정수\n",
        "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "\n",
        "    # 만약 현재 예측한 단어가 종료 토큰이라면 for문을 종료\n",
        "    if tf.equal(predicted_id, END_TOKEN[0]):\n",
        "      break\n",
        "\n",
        "    # 예측한 단어들은 지속적으로 output_sequence에 추가됩니다.\n",
        "    # 이 output_sequence는 다시 디코더의 입력이 됩니다.\n",
        "    output_sequence = tf.concat([output_sequence, predicted_id], axis=-1)\n",
        "\n",
        "  return tf.squeeze(output_sequence, axis=0)"
      ],
      "metadata": {
        "id": "TcSIAxxiQQtp"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_generation(sentence):\n",
        "  # 입력 문장에 대해서 디코더를 동작 시켜 예측된 정수 시퀀스를 리턴받습니다.\n",
        "  prediction = decoder_inference(sentence)\n",
        "\n",
        "  # 정수 시퀀스를 다시 텍스트 시퀀스로 변환합니다.\n",
        "  predicted_sentence = tokenizer.decode(\n",
        "      [i for i in prediction if i < tokenizer.vocab_size])\n",
        "\n",
        "  print('입력 : {}'.format(sentence))\n",
        "  print('출력 : {}'.format(predicted_sentence))\n",
        "\n",
        "  return predicted_sentence"
      ],
      "metadata": {
        "id": "LjoBo1ovLISh"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_question_list = [\n",
        "    \"안녕 오랜만이야\",\n",
        "    \"잘 지냈어?\",\n",
        "    \"뭐 먹을까\",\n",
        "    \"메뉴 추천해줘\",\n",
        "    \"어제 뭐 먹었어?\",\n",
        "    \"나랑 놀자\",\n",
        "    \"어디 가고 싶어?\",\n",
        "    \"널 만나서 기뻐\",\n",
        "    \"웃어줄래\",\n",
        "    \"삶은 뭘까?\",\n",
        "    \"인생살이 왜이리 힘드냐\",\n",
        "    \"이제 그만 쉬고 싶어\",\n",
        "    \"너무 고독하다\",\n",
        "    \"죽으면 어떻게 될까\",\n",
        "    \"내가 죽으면 슬퍼해 줄거야?\",\n",
        "    \"도망가고 싶다\",\n",
        "    \"우리 마지막이야\",\n",
        "    \"나 간다 잘 지내 안녕\",\n",
        "    \"넌 최고였어\",\n",
        "    \"나 잊지마\",\n",
        "    \"나는 심장이 없어\"]"
      ],
      "metadata": {
        "id": "VUcqdfjkLIM1"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_answers(input_question_list):\n",
        "    for input_qusetion in input_question_list:\n",
        "        sentence_generation(input_qusetion)\n",
        "        print()"
      ],
      "metadata": {
        "id": "Tvc0Nw6GLMzD"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##모델 평가"
      ],
      "metadata": {
        "id": "BrmQi3ZYLP-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 200\n",
        "model.fit(dataset, epochs=EPOCHS, verbose=1)"
      ],
      "metadata": {
        "id": "w3jgXPjjLejc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "363ffc88-8e26-4295-e639-4cd59ed6f488"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.0416 - accuracy: 0.1650\n",
            "Epoch 2/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0407 - accuracy: 0.1650\n",
            "Epoch 3/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0365 - accuracy: 0.1660\n",
            "Epoch 4/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0315 - accuracy: 0.1670\n",
            "Epoch 5/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0278 - accuracy: 0.1681\n",
            "Epoch 6/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0258 - accuracy: 0.1686\n",
            "Epoch 7/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0224 - accuracy: 0.1693\n",
            "Epoch 8/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0207 - accuracy: 0.1698\n",
            "Epoch 9/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.0189 - accuracy: 0.1702\n",
            "Epoch 10/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0166 - accuracy: 0.1708\n",
            "Epoch 11/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0161 - accuracy: 0.1711\n",
            "Epoch 12/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0151 - accuracy: 0.1712\n",
            "Epoch 13/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0133 - accuracy: 0.1717\n",
            "Epoch 14/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0131 - accuracy: 0.1717\n",
            "Epoch 15/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0120 - accuracy: 0.1721\n",
            "Epoch 16/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0111 - accuracy: 0.1723\n",
            "Epoch 17/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0103 - accuracy: 0.1725\n",
            "Epoch 18/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0099 - accuracy: 0.1726\n",
            "Epoch 19/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0093 - accuracy: 0.1727\n",
            "Epoch 20/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0094 - accuracy: 0.1727\n",
            "Epoch 21/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0083 - accuracy: 0.1730\n",
            "Epoch 22/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0083 - accuracy: 0.1730\n",
            "Epoch 23/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0081 - accuracy: 0.1731\n",
            "Epoch 24/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0073 - accuracy: 0.1732\n",
            "Epoch 25/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0071 - accuracy: 0.1732\n",
            "Epoch 26/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0070 - accuracy: 0.1733\n",
            "Epoch 27/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0062 - accuracy: 0.1735\n",
            "Epoch 28/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0064 - accuracy: 0.1734\n",
            "Epoch 29/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.0062 - accuracy: 0.1735\n",
            "Epoch 30/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.0059 - accuracy: 0.1735\n",
            "Epoch 31/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0057 - accuracy: 0.1736\n",
            "Epoch 32/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0056 - accuracy: 0.1735\n",
            "Epoch 33/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0054 - accuracy: 0.1737\n",
            "Epoch 34/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0050 - accuracy: 0.1737\n",
            "Epoch 35/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0049 - accuracy: 0.1737\n",
            "Epoch 36/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0051 - accuracy: 0.1737\n",
            "Epoch 37/200\n",
            "185/185 [==============================] - 11s 62ms/step - loss: 0.0048 - accuracy: 0.1738\n",
            "Epoch 38/200\n",
            "185/185 [==============================] - 11s 60ms/step - loss: 0.0046 - accuracy: 0.1738\n",
            "Epoch 39/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0045 - accuracy: 0.1738\n",
            "Epoch 40/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0045 - accuracy: 0.1738\n",
            "Epoch 41/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0041 - accuracy: 0.1739\n",
            "Epoch 42/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0043 - accuracy: 0.1738\n",
            "Epoch 43/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0040 - accuracy: 0.1739\n",
            "Epoch 44/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.0040 - accuracy: 0.1739\n",
            "Epoch 45/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.0041 - accuracy: 0.1739\n",
            "Epoch 46/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0040 - accuracy: 0.1739\n",
            "Epoch 47/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.0037 - accuracy: 0.1740\n",
            "Epoch 48/200\n",
            "185/185 [==============================] - 11s 60ms/step - loss: 0.0036 - accuracy: 0.1740\n",
            "Epoch 49/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.0032 - accuracy: 0.1741\n",
            "Epoch 50/200\n",
            "185/185 [==============================] - 11s 60ms/step - loss: 0.0036 - accuracy: 0.1740\n",
            "Epoch 51/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.0035 - accuracy: 0.1740\n",
            "Epoch 52/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0031 - accuracy: 0.1740\n",
            "Epoch 53/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0033 - accuracy: 0.1740\n",
            "Epoch 54/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0031 - accuracy: 0.1740\n",
            "Epoch 55/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0031 - accuracy: 0.1741\n",
            "Epoch 56/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.0029 - accuracy: 0.1742\n",
            "Epoch 57/200\n",
            "185/185 [==============================] - 11s 60ms/step - loss: 0.0028 - accuracy: 0.1741\n",
            "Epoch 58/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.0027 - accuracy: 0.1741\n",
            "Epoch 59/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0029 - accuracy: 0.1741\n",
            "Epoch 60/200\n",
            "185/185 [==============================] - 11s 61ms/step - loss: 0.0029 - accuracy: 0.1741\n",
            "Epoch 61/200\n",
            "185/185 [==============================] - 11s 60ms/step - loss: 0.0028 - accuracy: 0.1741\n",
            "Epoch 62/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0026 - accuracy: 0.1741\n",
            "Epoch 63/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.0026 - accuracy: 0.1741\n",
            "Epoch 64/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.0027 - accuracy: 0.1741\n",
            "Epoch 65/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.0024 - accuracy: 0.1741\n",
            "Epoch 66/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.0022 - accuracy: 0.1742\n",
            "Epoch 67/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.0023 - accuracy: 0.1742\n",
            "Epoch 68/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0025 - accuracy: 0.1741\n",
            "Epoch 69/200\n",
            "185/185 [==============================] - 11s 61ms/step - loss: 0.0024 - accuracy: 0.1741\n",
            "Epoch 70/200\n",
            "185/185 [==============================] - 11s 60ms/step - loss: 0.0023 - accuracy: 0.1742\n",
            "Epoch 71/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0022 - accuracy: 0.1742\n",
            "Epoch 72/200\n",
            "185/185 [==============================] - 11s 60ms/step - loss: 0.0022 - accuracy: 0.1742\n",
            "Epoch 73/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.0022 - accuracy: 0.1742\n",
            "Epoch 74/200\n",
            "185/185 [==============================] - 11s 61ms/step - loss: 0.0021 - accuracy: 0.1742\n",
            "Epoch 75/200\n",
            "185/185 [==============================] - 11s 60ms/step - loss: 0.0019 - accuracy: 0.1742\n",
            "Epoch 76/200\n",
            "185/185 [==============================] - 11s 60ms/step - loss: 0.0022 - accuracy: 0.1742\n",
            "Epoch 77/200\n",
            "185/185 [==============================] - 11s 60ms/step - loss: 0.0020 - accuracy: 0.1742\n",
            "Epoch 78/200\n",
            "185/185 [==============================] - 11s 61ms/step - loss: 0.0021 - accuracy: 0.1742\n",
            "Epoch 79/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0020 - accuracy: 0.1742\n",
            "Epoch 80/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.0020 - accuracy: 0.1742\n",
            "Epoch 81/200\n",
            "185/185 [==============================] - 11s 61ms/step - loss: 0.0019 - accuracy: 0.1742\n",
            "Epoch 82/200\n",
            "185/185 [==============================] - 11s 60ms/step - loss: 0.0019 - accuracy: 0.1742\n",
            "Epoch 83/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0019 - accuracy: 0.1742\n",
            "Epoch 84/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.0018 - accuracy: 0.1742\n",
            "Epoch 85/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.0021 - accuracy: 0.1742\n",
            "Epoch 86/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.0017 - accuracy: 0.1742\n",
            "Epoch 87/200\n",
            "185/185 [==============================] - 11s 60ms/step - loss: 0.0018 - accuracy: 0.1742\n",
            "Epoch 88/200\n",
            "185/185 [==============================] - 11s 60ms/step - loss: 0.0017 - accuracy: 0.1743\n",
            "Epoch 89/200\n",
            "185/185 [==============================] - 11s 60ms/step - loss: 0.0016 - accuracy: 0.1743\n",
            "Epoch 90/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.0018 - accuracy: 0.1742\n",
            "Epoch 91/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.0017 - accuracy: 0.1742\n",
            "Epoch 92/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0018 - accuracy: 0.1742\n",
            "Epoch 93/200\n",
            "185/185 [==============================] - 11s 61ms/step - loss: 0.0015 - accuracy: 0.1743\n",
            "Epoch 94/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.0018 - accuracy: 0.1742\n",
            "Epoch 95/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0016 - accuracy: 0.1743\n",
            "Epoch 96/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.0015 - accuracy: 0.1743\n",
            "Epoch 97/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0017 - accuracy: 0.1743\n",
            "Epoch 98/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0016 - accuracy: 0.1743\n",
            "Epoch 99/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0016 - accuracy: 0.1743\n",
            "Epoch 100/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0015 - accuracy: 0.1743\n",
            "Epoch 101/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0016 - accuracy: 0.1743\n",
            "Epoch 102/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.0016 - accuracy: 0.1742\n",
            "Epoch 103/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0015 - accuracy: 0.1743\n",
            "Epoch 104/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0015 - accuracy: 0.1743\n",
            "Epoch 105/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0016 - accuracy: 0.1743\n",
            "Epoch 106/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0016 - accuracy: 0.1742\n",
            "Epoch 107/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0014 - accuracy: 0.1743\n",
            "Epoch 108/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0016 - accuracy: 0.1743\n",
            "Epoch 109/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0016 - accuracy: 0.1742\n",
            "Epoch 110/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0015 - accuracy: 0.1743\n",
            "Epoch 111/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.0017 - accuracy: 0.1742\n",
            "Epoch 112/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0015 - accuracy: 0.1743\n",
            "Epoch 113/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.0015 - accuracy: 0.1743\n",
            "Epoch 114/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0015 - accuracy: 0.1743\n",
            "Epoch 115/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0015 - accuracy: 0.1743\n",
            "Epoch 116/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0015 - accuracy: 0.1743\n",
            "Epoch 117/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0014 - accuracy: 0.1743\n",
            "Epoch 118/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0013 - accuracy: 0.1743\n",
            "Epoch 119/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0014 - accuracy: 0.1743\n",
            "Epoch 120/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0012 - accuracy: 0.1743\n",
            "Epoch 121/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.0012 - accuracy: 0.1743\n",
            "Epoch 122/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0013 - accuracy: 0.1743\n",
            "Epoch 123/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0012 - accuracy: 0.1743\n",
            "Epoch 124/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0012 - accuracy: 0.1743\n",
            "Epoch 125/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0014 - accuracy: 0.1743\n",
            "Epoch 126/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0012 - accuracy: 0.1743\n",
            "Epoch 127/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0013 - accuracy: 0.1743\n",
            "Epoch 128/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0012 - accuracy: 0.1743\n",
            "Epoch 129/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0012 - accuracy: 0.1743\n",
            "Epoch 130/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.0013 - accuracy: 0.1743\n",
            "Epoch 131/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0012 - accuracy: 0.1743\n",
            "Epoch 132/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0012 - accuracy: 0.1743\n",
            "Epoch 133/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0012 - accuracy: 0.1743\n",
            "Epoch 134/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0012 - accuracy: 0.1743\n",
            "Epoch 135/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0012 - accuracy: 0.1743\n",
            "Epoch 136/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0013 - accuracy: 0.1743\n",
            "Epoch 137/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0013 - accuracy: 0.1743\n",
            "Epoch 138/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0011 - accuracy: 0.1743\n",
            "Epoch 139/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0012 - accuracy: 0.1743\n",
            "Epoch 140/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0012 - accuracy: 0.1743\n",
            "Epoch 141/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0013 - accuracy: 0.1743\n",
            "Epoch 142/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0013 - accuracy: 0.1743\n",
            "Epoch 143/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0012 - accuracy: 0.1743\n",
            "Epoch 144/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0013 - accuracy: 0.1743\n",
            "Epoch 145/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0012 - accuracy: 0.1743\n",
            "Epoch 146/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0012 - accuracy: 0.1743\n",
            "Epoch 147/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0012 - accuracy: 0.1743\n",
            "Epoch 148/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0012 - accuracy: 0.1743\n",
            "Epoch 149/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0011 - accuracy: 0.1743\n",
            "Epoch 150/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0012 - accuracy: 0.1743\n",
            "Epoch 151/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.0010 - accuracy: 0.1743\n",
            "Epoch 152/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0012 - accuracy: 0.1743\n",
            "Epoch 153/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.0011 - accuracy: 0.1743\n",
            "Epoch 154/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0011 - accuracy: 0.1743\n",
            "Epoch 155/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0012 - accuracy: 0.1743\n",
            "Epoch 156/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0011 - accuracy: 0.1743\n",
            "Epoch 157/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0012 - accuracy: 0.1743\n",
            "Epoch 158/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.0011 - accuracy: 0.1743\n",
            "Epoch 159/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0010 - accuracy: 0.1743\n",
            "Epoch 160/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0012 - accuracy: 0.1743\n",
            "Epoch 161/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.0012 - accuracy: 0.1743\n",
            "Epoch 162/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.0011 - accuracy: 0.1743\n",
            "Epoch 163/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.0012 - accuracy: 0.1743\n",
            "Epoch 164/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.0011 - accuracy: 0.1743\n",
            "Epoch 165/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.0010 - accuracy: 0.1743\n",
            "Epoch 166/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.0011 - accuracy: 0.1743\n",
            "Epoch 167/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 9.4795e-04 - accuracy: 0.1743\n",
            "Epoch 168/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.0011 - accuracy: 0.1744\n",
            "Epoch 169/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.0011 - accuracy: 0.1743\n",
            "Epoch 170/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.0010 - accuracy: 0.1743\n",
            "Epoch 171/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.0011 - accuracy: 0.1743\n",
            "Epoch 172/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.0011 - accuracy: 0.1743\n",
            "Epoch 173/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0010 - accuracy: 0.1743\n",
            "Epoch 174/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.0012 - accuracy: 0.1743\n",
            "Epoch 175/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.0011 - accuracy: 0.1743\n",
            "Epoch 176/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 9.8052e-04 - accuracy: 0.1743\n",
            "Epoch 177/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.0012 - accuracy: 0.1743\n",
            "Epoch 178/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.0010 - accuracy: 0.1743\n",
            "Epoch 179/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 9.8430e-04 - accuracy: 0.1743\n",
            "Epoch 180/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.0010 - accuracy: 0.1743\n",
            "Epoch 181/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.0011 - accuracy: 0.1743\n",
            "Epoch 182/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.0011 - accuracy: 0.1743\n",
            "Epoch 183/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.0010 - accuracy: 0.1743\n",
            "Epoch 184/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 9.3715e-04 - accuracy: 0.1743\n",
            "Epoch 185/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 9.7737e-04 - accuracy: 0.1743\n",
            "Epoch 186/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.0011 - accuracy: 0.1743\n",
            "Epoch 187/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.0010 - accuracy: 0.1743\n",
            "Epoch 188/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.0011 - accuracy: 0.1743\n",
            "Epoch 189/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0011 - accuracy: 0.1743\n",
            "Epoch 190/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 9.4683e-04 - accuracy: 0.1743\n",
            "Epoch 191/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.0010 - accuracy: 0.1743\n",
            "Epoch 192/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 9.7083e-04 - accuracy: 0.1743\n",
            "Epoch 193/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.0010 - accuracy: 0.1743\n",
            "Epoch 194/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.0011 - accuracy: 0.1743\n",
            "Epoch 195/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 9.4429e-04 - accuracy: 0.1743\n",
            "Epoch 196/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.0010 - accuracy: 0.1743\n",
            "Epoch 197/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.0012 - accuracy: 0.1743\n",
            "Epoch 198/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.0010 - accuracy: 0.1743\n",
            "Epoch 199/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 9.7713e-04 - accuracy: 0.1743\n",
            "Epoch 200/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.0011 - accuracy: 0.1743\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1ebfeeaad0>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_answers(input_question_list)"
      ],
      "metadata": {
        "id": "JSw2YMLGLPk5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd4074f5-9242-4381-dd2d-059d01f2b54f"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력 : 안녕 오랜만이야\n",
            "출력 : 오랜만이에요 .\n",
            "\n",
            "입력 : 잘 지냈어?\n",
            "출력 : 안부를 물어주시다니 감사합니다 .\n",
            "\n",
            "입력 : 뭐 먹을까\n",
            "출력 : 좀 먹어도 괜찮아요 .\n",
            "\n",
            "입력 : 메뉴 추천해줘\n",
            "출력 : 같이 이야기 해요 .\n",
            "\n",
            "입력 : 어제 뭐 먹었어?\n",
            "출력 : 저는 배터리가 밥이예요 .\n",
            "\n",
            "입력 : 나랑 놀자\n",
            "출력 : 지금 그러고 있어요 .\n",
            "\n",
            "입력 : 어디 가고 싶어?\n",
            "출력 : 지금 그러고 있어요 .\n",
            "\n",
            "입력 : 널 만나서 기뻐\n",
            "출력 : 꿈꾸던 여행이네요 .\n",
            "\n",
            "입력 : 웃어줄래\n",
            "출력 : 기대를 많이 하는 건 좋지 않아요 .\n",
            "\n",
            "입력 : 삶은 뭘까?\n",
            "출력 : 지칠 때는 쉬어도 돼요 .\n",
            "\n",
            "입력 : 인생살이 왜이리 힘드냐\n",
            "출력 : 어떤 일이냐에 따라 다를 거 같아요 .\n",
            "\n",
            "입력 : 이제 그만 쉬고 싶어\n",
            "출력 : 잠깐 바람 쐬고 오세요 .\n",
            "\n",
            "입력 : 너무 고독하다\n",
            "출력 : 혼자가 아니에요 .\n",
            "\n",
            "입력 : 죽으면 어떻게 될까\n",
            "출력 : 그게 서로에게 좋았던 선택일 거예요 .\n",
            "\n",
            "입력 : 내가 죽으면 슬퍼해 줄거야?\n",
            "출력 : 그게 사랑의 증거가 되겠네요 .\n",
            "\n",
            "입력 : 도망가고 싶다\n",
            "출력 : 제가 알면 제가 먼저 했을 거예요 .\n",
            "\n",
            "입력 : 우리 마지막이야\n",
            "출력 : 마지막이 아닐 지도 몰라요 .\n",
            "\n",
            "입력 : 나 간다 잘 지내 안녕\n",
            "출력 : 저랑 놀아서 그래요 .\n",
            "\n",
            "입력 : 넌 최고였어\n",
            "출력 : 당신도 예뻐요 .\n",
            "\n",
            "입력 : 나 잊지마\n",
            "출력 : 때론 잊어버리는 것이 좋을 때도 있어요 .\n",
            "\n",
            "입력 : 나는 심장이 없어\n",
            "출력 : 심호흡 해보세요 .\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "BUFFER_SIZE = 30000\n",
        "\n",
        "# 디코더는 이전의 target을 다음의 input으로 사용합니다.\n",
        "# 이에 따라 outputs에서는 START_TOKEN을 제거하겠습니다.\n",
        "dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    {\n",
        "        'inputs': questions,\n",
        "        'dec_inputs': answers[:, :-1]\n",
        "    },\n",
        "    {\n",
        "        'outputs': answers[:, 1:]\n",
        "    },\n",
        "))\n",
        "\n",
        "dataset = dataset.cache()\n",
        "dataset = dataset.shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE)\n",
        "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "metadata": {
        "id": "kgqrX8wPLzuE"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##모델생성"
      ],
      "metadata": {
        "id": "ITDla-xXL3wZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# 하이퍼파라미터\n",
        "NUM_LAYERS = 2 # 인코더와 디코더의 층의 개수\n",
        "D_MODEL = 512 # 인코더와 디코더 내부의 입, 출력의 고정 차원\n",
        "NUM_HEADS = 16 # 멀티 헤드 어텐션에서의 헤드 수 \n",
        "UNITS = 1024 # 피드 포워드 신경망의 은닉층의 크기\n",
        "DROPOUT = 0.4 # 드롭아웃의 비율\n",
        "\n",
        "model = transformer(\n",
        "    vocab_size=VOCAB_SIZE,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    units=UNITS,\n",
        "    d_model=D_MODEL,\n",
        "    num_heads=NUM_HEADS,\n",
        "    dropout=DROPOUT)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "XsQiB9NAL3Sb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85a0dad6-5a2e-48c5-e05c-ccb6bf224fb1"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"transformer\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " inputs (InputLayer)            [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " dec_inputs (InputLayer)        [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " enc_padding_mask (Lambda)      (None, 1, 1, None)   0           ['inputs[0][0]']                 \n",
            "                                                                                                  \n",
            " encoder (Functional)           (None, None, 512)    8367616     ['inputs[0][0]',                 \n",
            "                                                                  'enc_padding_mask[0][0]']       \n",
            "                                                                                                  \n",
            " look_ahead_mask (Lambda)       (None, 1, None, Non  0           ['dec_inputs[0][0]']             \n",
            "                                e)                                                                \n",
            "                                                                                                  \n",
            " dec_padding_mask (Lambda)      (None, 1, 1, None)   0           ['inputs[0][0]']                 \n",
            "                                                                                                  \n",
            " decoder (Functional)           (None, None, 512)    10470912    ['dec_inputs[0][0]',             \n",
            "                                                                  'encoder[0][0]',                \n",
            "                                                                  'look_ahead_mask[0][0]',        \n",
            "                                                                  'dec_padding_mask[0][0]']       \n",
            "                                                                                                  \n",
            " outputs (Dense)                (None, None, 8129)   4170177     ['decoder[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 23,008,705\n",
            "Trainable params: 23,008,705\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = CustomSchedule(D_MODEL)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(\n",
        "    learning_rate, beta_1=0.5, beta_2=0.98, epsilon=1e-9)\n",
        "\n",
        "def accuracy(y_true, y_pred):\n",
        "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
        "  return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])"
      ],
      "metadata": {
        "id": "iN7FHy3JL6py"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 500\n",
        "model.fit(dataset, epochs=EPOCHS, verbose=1)"
      ],
      "metadata": {
        "id": "0xaikI5iL85x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "634111c9-5f01-4714-f683-a28b222f010f"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "370/370 [==============================] - 29s 63ms/step - loss: 1.2497 - accuracy: 0.0336\n",
            "Epoch 2/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 1.0014 - accuracy: 0.0505\n",
            "Epoch 3/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.9414 - accuracy: 0.0535\n",
            "Epoch 4/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.8899 - accuracy: 0.0565\n",
            "Epoch 5/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.8342 - accuracy: 0.0600\n",
            "Epoch 6/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.7698 - accuracy: 0.0642\n",
            "Epoch 7/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.6990 - accuracy: 0.0702\n",
            "Epoch 8/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.6274 - accuracy: 0.0775\n",
            "Epoch 9/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.5617 - accuracy: 0.0839\n",
            "Epoch 10/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.5057 - accuracy: 0.0908\n",
            "Epoch 11/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.4608 - accuracy: 0.0964\n",
            "Epoch 12/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.4200 - accuracy: 0.1020\n",
            "Epoch 13/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.3859 - accuracy: 0.1071\n",
            "Epoch 14/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.3570 - accuracy: 0.1122\n",
            "Epoch 15/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.3369 - accuracy: 0.1161\n",
            "Epoch 16/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.3192 - accuracy: 0.1194\n",
            "Epoch 17/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.3067 - accuracy: 0.1219\n",
            "Epoch 18/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.2974 - accuracy: 0.1239\n",
            "Epoch 19/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.2883 - accuracy: 0.1255\n",
            "Epoch 20/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.2806 - accuracy: 0.1271\n",
            "Epoch 21/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.2750 - accuracy: 0.1277\n",
            "Epoch 22/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.2689 - accuracy: 0.1290\n",
            "Epoch 23/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.2639 - accuracy: 0.1298\n",
            "Epoch 24/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.2596 - accuracy: 0.1305\n",
            "Epoch 25/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.2554 - accuracy: 0.1310\n",
            "Epoch 26/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.2515 - accuracy: 0.1317\n",
            "Epoch 27/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.2470 - accuracy: 0.1325\n",
            "Epoch 28/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.2430 - accuracy: 0.1331\n",
            "Epoch 29/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.2391 - accuracy: 0.1338\n",
            "Epoch 30/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.2371 - accuracy: 0.1343\n",
            "Epoch 31/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.2335 - accuracy: 0.1345\n",
            "Epoch 32/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.2307 - accuracy: 0.1353\n",
            "Epoch 33/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.2284 - accuracy: 0.1355\n",
            "Epoch 34/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.2254 - accuracy: 0.1360\n",
            "Epoch 35/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.2233 - accuracy: 0.1363\n",
            "Epoch 36/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.2209 - accuracy: 0.1367\n",
            "Epoch 37/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.2182 - accuracy: 0.1371\n",
            "Epoch 38/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.2159 - accuracy: 0.1374\n",
            "Epoch 39/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.2149 - accuracy: 0.1377\n",
            "Epoch 40/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.2127 - accuracy: 0.1380\n",
            "Epoch 41/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.2104 - accuracy: 0.1383\n",
            "Epoch 42/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.2085 - accuracy: 0.1388\n",
            "Epoch 43/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.2073 - accuracy: 0.1388\n",
            "Epoch 44/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.2053 - accuracy: 0.1392\n",
            "Epoch 45/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.2036 - accuracy: 0.1395\n",
            "Epoch 46/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.2019 - accuracy: 0.1398\n",
            "Epoch 47/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.2007 - accuracy: 0.1399\n",
            "Epoch 48/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1994 - accuracy: 0.1402\n",
            "Epoch 49/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1986 - accuracy: 0.1404\n",
            "Epoch 50/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1968 - accuracy: 0.1407\n",
            "Epoch 51/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1958 - accuracy: 0.1407\n",
            "Epoch 52/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1938 - accuracy: 0.1410\n",
            "Epoch 53/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1934 - accuracy: 0.1410\n",
            "Epoch 54/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1914 - accuracy: 0.1414\n",
            "Epoch 55/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1908 - accuracy: 0.1417\n",
            "Epoch 56/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1898 - accuracy: 0.1418\n",
            "Epoch 57/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1894 - accuracy: 0.1417\n",
            "Epoch 58/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1881 - accuracy: 0.1419\n",
            "Epoch 59/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1868 - accuracy: 0.1421\n",
            "Epoch 60/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1856 - accuracy: 0.1423\n",
            "Epoch 61/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1851 - accuracy: 0.1423\n",
            "Epoch 62/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1836 - accuracy: 0.1426\n",
            "Epoch 63/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1831 - accuracy: 0.1427\n",
            "Epoch 64/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1816 - accuracy: 0.1428\n",
            "Epoch 65/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1811 - accuracy: 0.1429\n",
            "Epoch 66/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1797 - accuracy: 0.1432\n",
            "Epoch 67/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1792 - accuracy: 0.1432\n",
            "Epoch 68/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1784 - accuracy: 0.1432\n",
            "Epoch 69/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1775 - accuracy: 0.1434\n",
            "Epoch 70/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1770 - accuracy: 0.1434\n",
            "Epoch 71/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1762 - accuracy: 0.1435\n",
            "Epoch 72/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1752 - accuracy: 0.1438\n",
            "Epoch 73/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1750 - accuracy: 0.1438\n",
            "Epoch 74/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1744 - accuracy: 0.1437\n",
            "Epoch 75/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1731 - accuracy: 0.1440\n",
            "Epoch 76/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1727 - accuracy: 0.1439\n",
            "Epoch 77/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1721 - accuracy: 0.1440\n",
            "Epoch 78/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1715 - accuracy: 0.1441\n",
            "Epoch 79/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1707 - accuracy: 0.1443\n",
            "Epoch 80/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1708 - accuracy: 0.1444\n",
            "Epoch 81/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1703 - accuracy: 0.1445\n",
            "Epoch 82/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1692 - accuracy: 0.1445\n",
            "Epoch 83/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1683 - accuracy: 0.1445\n",
            "Epoch 84/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1677 - accuracy: 0.1448\n",
            "Epoch 85/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1667 - accuracy: 0.1449\n",
            "Epoch 86/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1665 - accuracy: 0.1448\n",
            "Epoch 87/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1663 - accuracy: 0.1449\n",
            "Epoch 88/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1646 - accuracy: 0.1452\n",
            "Epoch 89/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1642 - accuracy: 0.1451\n",
            "Epoch 90/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1644 - accuracy: 0.1453\n",
            "Epoch 91/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1639 - accuracy: 0.1452\n",
            "Epoch 92/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1629 - accuracy: 0.1454\n",
            "Epoch 93/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1629 - accuracy: 0.1453\n",
            "Epoch 94/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1623 - accuracy: 0.1454\n",
            "Epoch 95/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1615 - accuracy: 0.1455\n",
            "Epoch 96/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1617 - accuracy: 0.1454\n",
            "Epoch 97/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1605 - accuracy: 0.1457\n",
            "Epoch 98/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1600 - accuracy: 0.1457\n",
            "Epoch 99/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1602 - accuracy: 0.1457\n",
            "Epoch 100/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1589 - accuracy: 0.1459\n",
            "Epoch 101/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1590 - accuracy: 0.1458\n",
            "Epoch 102/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1585 - accuracy: 0.1459\n",
            "Epoch 103/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1578 - accuracy: 0.1461\n",
            "Epoch 104/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1575 - accuracy: 0.1461\n",
            "Epoch 105/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1569 - accuracy: 0.1461\n",
            "Epoch 106/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1568 - accuracy: 0.1460\n",
            "Epoch 107/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1563 - accuracy: 0.1462\n",
            "Epoch 108/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1558 - accuracy: 0.1462\n",
            "Epoch 109/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1549 - accuracy: 0.1463\n",
            "Epoch 110/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1545 - accuracy: 0.1464\n",
            "Epoch 111/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1543 - accuracy: 0.1463\n",
            "Epoch 112/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1542 - accuracy: 0.1465\n",
            "Epoch 113/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1535 - accuracy: 0.1465\n",
            "Epoch 114/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1533 - accuracy: 0.1466\n",
            "Epoch 115/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1525 - accuracy: 0.1466\n",
            "Epoch 116/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1526 - accuracy: 0.1466\n",
            "Epoch 117/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1518 - accuracy: 0.1466\n",
            "Epoch 118/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1519 - accuracy: 0.1466\n",
            "Epoch 119/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1517 - accuracy: 0.1466\n",
            "Epoch 120/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1509 - accuracy: 0.1468\n",
            "Epoch 121/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1504 - accuracy: 0.1469\n",
            "Epoch 122/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1509 - accuracy: 0.1468\n",
            "Epoch 123/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1501 - accuracy: 0.1468\n",
            "Epoch 124/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1502 - accuracy: 0.1469\n",
            "Epoch 125/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1492 - accuracy: 0.1469\n",
            "Epoch 126/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1487 - accuracy: 0.1470\n",
            "Epoch 127/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1487 - accuracy: 0.1470\n",
            "Epoch 128/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1483 - accuracy: 0.1470\n",
            "Epoch 129/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1482 - accuracy: 0.1471\n",
            "Epoch 130/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1476 - accuracy: 0.1471\n",
            "Epoch 131/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1474 - accuracy: 0.1473\n",
            "Epoch 132/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1469 - accuracy: 0.1471\n",
            "Epoch 133/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1469 - accuracy: 0.1474\n",
            "Epoch 134/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1469 - accuracy: 0.1473\n",
            "Epoch 135/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1468 - accuracy: 0.1473\n",
            "Epoch 136/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1458 - accuracy: 0.1475\n",
            "Epoch 137/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1466 - accuracy: 0.1473\n",
            "Epoch 138/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1459 - accuracy: 0.1475\n",
            "Epoch 139/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1457 - accuracy: 0.1473\n",
            "Epoch 140/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1452 - accuracy: 0.1475\n",
            "Epoch 141/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1446 - accuracy: 0.1477\n",
            "Epoch 142/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1445 - accuracy: 0.1477\n",
            "Epoch 143/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1437 - accuracy: 0.1478\n",
            "Epoch 144/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1437 - accuracy: 0.1476\n",
            "Epoch 145/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1437 - accuracy: 0.1477\n",
            "Epoch 146/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1435 - accuracy: 0.1477\n",
            "Epoch 147/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1427 - accuracy: 0.1479\n",
            "Epoch 148/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1425 - accuracy: 0.1478\n",
            "Epoch 149/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1425 - accuracy: 0.1477\n",
            "Epoch 150/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1423 - accuracy: 0.1479\n",
            "Epoch 151/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1420 - accuracy: 0.1479\n",
            "Epoch 152/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1416 - accuracy: 0.1481\n",
            "Epoch 153/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1412 - accuracy: 0.1478\n",
            "Epoch 154/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1411 - accuracy: 0.1480\n",
            "Epoch 155/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1410 - accuracy: 0.1480\n",
            "Epoch 156/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1410 - accuracy: 0.1482\n",
            "Epoch 157/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1403 - accuracy: 0.1481\n",
            "Epoch 158/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1403 - accuracy: 0.1481\n",
            "Epoch 159/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1401 - accuracy: 0.1483\n",
            "Epoch 160/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1399 - accuracy: 0.1481\n",
            "Epoch 161/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1386 - accuracy: 0.1483\n",
            "Epoch 162/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1392 - accuracy: 0.1483\n",
            "Epoch 163/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1388 - accuracy: 0.1483\n",
            "Epoch 164/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1386 - accuracy: 0.1483\n",
            "Epoch 165/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1392 - accuracy: 0.1483\n",
            "Epoch 166/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1384 - accuracy: 0.1485\n",
            "Epoch 167/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1387 - accuracy: 0.1482\n",
            "Epoch 168/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1384 - accuracy: 0.1482\n",
            "Epoch 169/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1380 - accuracy: 0.1482\n",
            "Epoch 170/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1382 - accuracy: 0.1484\n",
            "Epoch 171/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1377 - accuracy: 0.1484\n",
            "Epoch 172/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1371 - accuracy: 0.1485\n",
            "Epoch 173/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1365 - accuracy: 0.1486\n",
            "Epoch 174/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1370 - accuracy: 0.1485\n",
            "Epoch 175/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1367 - accuracy: 0.1486\n",
            "Epoch 176/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1362 - accuracy: 0.1487\n",
            "Epoch 177/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1362 - accuracy: 0.1486\n",
            "Epoch 178/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1364 - accuracy: 0.1486\n",
            "Epoch 179/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1359 - accuracy: 0.1486\n",
            "Epoch 180/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1360 - accuracy: 0.1487\n",
            "Epoch 181/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1355 - accuracy: 0.1487\n",
            "Epoch 182/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1348 - accuracy: 0.1487\n",
            "Epoch 183/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1351 - accuracy: 0.1488\n",
            "Epoch 184/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1346 - accuracy: 0.1490\n",
            "Epoch 185/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1347 - accuracy: 0.1488\n",
            "Epoch 186/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1347 - accuracy: 0.1487\n",
            "Epoch 187/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1344 - accuracy: 0.1489\n",
            "Epoch 188/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1341 - accuracy: 0.1489\n",
            "Epoch 189/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1337 - accuracy: 0.1489\n",
            "Epoch 190/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1333 - accuracy: 0.1490\n",
            "Epoch 191/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1337 - accuracy: 0.1490\n",
            "Epoch 192/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1326 - accuracy: 0.1491\n",
            "Epoch 193/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1328 - accuracy: 0.1490\n",
            "Epoch 194/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1329 - accuracy: 0.1491\n",
            "Epoch 195/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1328 - accuracy: 0.1490\n",
            "Epoch 196/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1328 - accuracy: 0.1489\n",
            "Epoch 197/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1323 - accuracy: 0.1491\n",
            "Epoch 198/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1324 - accuracy: 0.1491\n",
            "Epoch 199/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1328 - accuracy: 0.1490\n",
            "Epoch 200/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1321 - accuracy: 0.1492\n",
            "Epoch 201/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1320 - accuracy: 0.1490\n",
            "Epoch 202/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1315 - accuracy: 0.1493\n",
            "Epoch 203/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1310 - accuracy: 0.1493\n",
            "Epoch 204/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1315 - accuracy: 0.1492\n",
            "Epoch 205/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1311 - accuracy: 0.1493\n",
            "Epoch 206/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1309 - accuracy: 0.1494\n",
            "Epoch 207/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1309 - accuracy: 0.1493\n",
            "Epoch 208/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1299 - accuracy: 0.1494\n",
            "Epoch 209/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1297 - accuracy: 0.1494\n",
            "Epoch 210/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1305 - accuracy: 0.1492\n",
            "Epoch 211/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1298 - accuracy: 0.1495\n",
            "Epoch 212/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1298 - accuracy: 0.1493\n",
            "Epoch 213/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1300 - accuracy: 0.1495\n",
            "Epoch 214/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1288 - accuracy: 0.1496\n",
            "Epoch 215/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1293 - accuracy: 0.1494\n",
            "Epoch 216/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1289 - accuracy: 0.1495\n",
            "Epoch 217/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1293 - accuracy: 0.1496\n",
            "Epoch 218/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1294 - accuracy: 0.1494\n",
            "Epoch 219/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1287 - accuracy: 0.1495\n",
            "Epoch 220/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1282 - accuracy: 0.1495\n",
            "Epoch 221/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1280 - accuracy: 0.1496\n",
            "Epoch 222/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1278 - accuracy: 0.1497\n",
            "Epoch 223/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1277 - accuracy: 0.1498\n",
            "Epoch 224/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1278 - accuracy: 0.1495\n",
            "Epoch 225/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1279 - accuracy: 0.1495\n",
            "Epoch 226/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1280 - accuracy: 0.1496\n",
            "Epoch 227/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1284 - accuracy: 0.1495\n",
            "Epoch 228/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1275 - accuracy: 0.1496\n",
            "Epoch 229/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1271 - accuracy: 0.1499\n",
            "Epoch 230/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1273 - accuracy: 0.1498\n",
            "Epoch 231/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1267 - accuracy: 0.1498\n",
            "Epoch 232/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1273 - accuracy: 0.1497\n",
            "Epoch 233/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1265 - accuracy: 0.1498\n",
            "Epoch 234/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1268 - accuracy: 0.1498\n",
            "Epoch 235/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1264 - accuracy: 0.1499\n",
            "Epoch 236/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1265 - accuracy: 0.1499\n",
            "Epoch 237/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1258 - accuracy: 0.1499\n",
            "Epoch 238/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1261 - accuracy: 0.1499\n",
            "Epoch 239/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1259 - accuracy: 0.1498\n",
            "Epoch 240/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1257 - accuracy: 0.1500\n",
            "Epoch 241/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1261 - accuracy: 0.1499\n",
            "Epoch 242/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1256 - accuracy: 0.1501\n",
            "Epoch 243/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1253 - accuracy: 0.1501\n",
            "Epoch 244/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1257 - accuracy: 0.1499\n",
            "Epoch 245/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1253 - accuracy: 0.1500\n",
            "Epoch 246/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1248 - accuracy: 0.1500\n",
            "Epoch 247/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1242 - accuracy: 0.1502\n",
            "Epoch 248/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1252 - accuracy: 0.1501\n",
            "Epoch 249/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1238 - accuracy: 0.1502\n",
            "Epoch 250/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1241 - accuracy: 0.1500\n",
            "Epoch 251/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1243 - accuracy: 0.1501\n",
            "Epoch 252/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1240 - accuracy: 0.1501\n",
            "Epoch 253/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1245 - accuracy: 0.1501\n",
            "Epoch 254/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1239 - accuracy: 0.1502\n",
            "Epoch 255/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1241 - accuracy: 0.1502\n",
            "Epoch 256/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1237 - accuracy: 0.1502\n",
            "Epoch 257/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1236 - accuracy: 0.1502\n",
            "Epoch 258/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1235 - accuracy: 0.1502\n",
            "Epoch 259/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1236 - accuracy: 0.1502\n",
            "Epoch 260/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1227 - accuracy: 0.1504\n",
            "Epoch 261/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1223 - accuracy: 0.1505\n",
            "Epoch 262/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1226 - accuracy: 0.1503\n",
            "Epoch 263/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1223 - accuracy: 0.1502\n",
            "Epoch 264/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1223 - accuracy: 0.1504\n",
            "Epoch 265/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1211 - accuracy: 0.1506\n",
            "Epoch 266/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1220 - accuracy: 0.1504\n",
            "Epoch 267/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1217 - accuracy: 0.1505\n",
            "Epoch 268/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1215 - accuracy: 0.1504\n",
            "Epoch 269/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1220 - accuracy: 0.1505\n",
            "Epoch 270/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1223 - accuracy: 0.1503\n",
            "Epoch 271/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1217 - accuracy: 0.1504\n",
            "Epoch 272/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1218 - accuracy: 0.1504\n",
            "Epoch 273/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1216 - accuracy: 0.1505\n",
            "Epoch 274/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1211 - accuracy: 0.1507\n",
            "Epoch 275/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1215 - accuracy: 0.1505\n",
            "Epoch 276/500\n",
            "370/370 [==============================] - 24s 64ms/step - loss: 0.1213 - accuracy: 0.1506\n",
            "Epoch 277/500\n",
            "370/370 [==============================] - 24s 64ms/step - loss: 0.1207 - accuracy: 0.1506\n",
            "Epoch 278/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1210 - accuracy: 0.1505\n",
            "Epoch 279/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1208 - accuracy: 0.1508\n",
            "Epoch 280/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1211 - accuracy: 0.1507\n",
            "Epoch 281/500\n",
            "370/370 [==============================] - 24s 64ms/step - loss: 0.1206 - accuracy: 0.1507\n",
            "Epoch 282/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1214 - accuracy: 0.1504\n",
            "Epoch 283/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1201 - accuracy: 0.1507\n",
            "Epoch 284/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1202 - accuracy: 0.1507\n",
            "Epoch 285/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1204 - accuracy: 0.1506\n",
            "Epoch 286/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1198 - accuracy: 0.1508\n",
            "Epoch 287/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1197 - accuracy: 0.1507\n",
            "Epoch 288/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1200 - accuracy: 0.1505\n",
            "Epoch 289/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1195 - accuracy: 0.1509\n",
            "Epoch 290/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1198 - accuracy: 0.1508\n",
            "Epoch 291/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1194 - accuracy: 0.1509\n",
            "Epoch 292/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1193 - accuracy: 0.1508\n",
            "Epoch 293/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1193 - accuracy: 0.1509\n",
            "Epoch 294/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1190 - accuracy: 0.1509\n",
            "Epoch 295/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1189 - accuracy: 0.1509\n",
            "Epoch 296/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1188 - accuracy: 0.1508\n",
            "Epoch 297/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1187 - accuracy: 0.1510\n",
            "Epoch 298/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1183 - accuracy: 0.1511\n",
            "Epoch 299/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1183 - accuracy: 0.1510\n",
            "Epoch 300/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1176 - accuracy: 0.1510\n",
            "Epoch 301/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1181 - accuracy: 0.1510\n",
            "Epoch 302/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1179 - accuracy: 0.1510\n",
            "Epoch 303/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1182 - accuracy: 0.1510\n",
            "Epoch 304/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1176 - accuracy: 0.1511\n",
            "Epoch 305/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1174 - accuracy: 0.1512\n",
            "Epoch 306/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1175 - accuracy: 0.1512\n",
            "Epoch 307/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1175 - accuracy: 0.1511\n",
            "Epoch 308/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1162 - accuracy: 0.1512\n",
            "Epoch 309/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1172 - accuracy: 0.1511\n",
            "Epoch 310/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1179 - accuracy: 0.1510\n",
            "Epoch 311/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1175 - accuracy: 0.1510\n",
            "Epoch 312/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1176 - accuracy: 0.1509\n",
            "Epoch 313/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1173 - accuracy: 0.1512\n",
            "Epoch 314/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1170 - accuracy: 0.1512\n",
            "Epoch 315/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1166 - accuracy: 0.1512\n",
            "Epoch 316/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1167 - accuracy: 0.1510\n",
            "Epoch 317/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1167 - accuracy: 0.1511\n",
            "Epoch 318/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1169 - accuracy: 0.1511\n",
            "Epoch 319/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1167 - accuracy: 0.1511\n",
            "Epoch 320/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1165 - accuracy: 0.1511\n",
            "Epoch 321/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1166 - accuracy: 0.1512\n",
            "Epoch 322/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1163 - accuracy: 0.1512\n",
            "Epoch 323/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1164 - accuracy: 0.1511\n",
            "Epoch 324/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1155 - accuracy: 0.1513\n",
            "Epoch 325/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1158 - accuracy: 0.1512\n",
            "Epoch 326/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1157 - accuracy: 0.1513\n",
            "Epoch 327/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1160 - accuracy: 0.1512\n",
            "Epoch 328/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1152 - accuracy: 0.1514\n",
            "Epoch 329/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1153 - accuracy: 0.1513\n",
            "Epoch 330/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1155 - accuracy: 0.1513\n",
            "Epoch 331/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1148 - accuracy: 0.1515\n",
            "Epoch 332/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1150 - accuracy: 0.1513\n",
            "Epoch 333/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1155 - accuracy: 0.1512\n",
            "Epoch 334/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1156 - accuracy: 0.1514\n",
            "Epoch 335/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1141 - accuracy: 0.1515\n",
            "Epoch 336/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1146 - accuracy: 0.1514\n",
            "Epoch 337/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1147 - accuracy: 0.1514\n",
            "Epoch 338/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1150 - accuracy: 0.1514\n",
            "Epoch 339/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1144 - accuracy: 0.1515\n",
            "Epoch 340/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1143 - accuracy: 0.1516\n",
            "Epoch 341/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1150 - accuracy: 0.1513\n",
            "Epoch 342/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1135 - accuracy: 0.1516\n",
            "Epoch 343/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1141 - accuracy: 0.1515\n",
            "Epoch 344/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1146 - accuracy: 0.1514\n",
            "Epoch 345/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1141 - accuracy: 0.1515\n",
            "Epoch 346/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1140 - accuracy: 0.1516\n",
            "Epoch 347/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1139 - accuracy: 0.1515\n",
            "Epoch 348/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1141 - accuracy: 0.1516\n",
            "Epoch 349/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1138 - accuracy: 0.1516\n",
            "Epoch 350/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1136 - accuracy: 0.1516\n",
            "Epoch 351/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1129 - accuracy: 0.1516\n",
            "Epoch 352/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1139 - accuracy: 0.1516\n",
            "Epoch 353/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1139 - accuracy: 0.1515\n",
            "Epoch 354/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1136 - accuracy: 0.1516\n",
            "Epoch 355/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1135 - accuracy: 0.1516\n",
            "Epoch 356/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1128 - accuracy: 0.1517\n",
            "Epoch 357/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1134 - accuracy: 0.1518\n",
            "Epoch 358/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1129 - accuracy: 0.1519\n",
            "Epoch 359/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1128 - accuracy: 0.1516\n",
            "Epoch 360/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1129 - accuracy: 0.1518\n",
            "Epoch 361/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1127 - accuracy: 0.1516\n",
            "Epoch 362/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1123 - accuracy: 0.1520\n",
            "Epoch 363/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1123 - accuracy: 0.1518\n",
            "Epoch 364/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1129 - accuracy: 0.1518\n",
            "Epoch 365/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1125 - accuracy: 0.1519\n",
            "Epoch 366/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1127 - accuracy: 0.1519\n",
            "Epoch 367/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1118 - accuracy: 0.1520\n",
            "Epoch 368/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1122 - accuracy: 0.1519\n",
            "Epoch 369/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1113 - accuracy: 0.1519\n",
            "Epoch 370/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1124 - accuracy: 0.1518\n",
            "Epoch 371/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1120 - accuracy: 0.1519\n",
            "Epoch 372/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1123 - accuracy: 0.1518\n",
            "Epoch 373/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1119 - accuracy: 0.1520\n",
            "Epoch 374/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1119 - accuracy: 0.1518\n",
            "Epoch 375/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1118 - accuracy: 0.1518\n",
            "Epoch 376/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1112 - accuracy: 0.1520\n",
            "Epoch 377/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1110 - accuracy: 0.1522\n",
            "Epoch 378/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1113 - accuracy: 0.1518\n",
            "Epoch 379/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1106 - accuracy: 0.1521\n",
            "Epoch 380/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1106 - accuracy: 0.1520\n",
            "Epoch 381/500\n",
            "370/370 [==============================] - 24s 64ms/step - loss: 0.1110 - accuracy: 0.1520\n",
            "Epoch 382/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1107 - accuracy: 0.1519\n",
            "Epoch 383/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1108 - accuracy: 0.1520\n",
            "Epoch 384/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1108 - accuracy: 0.1520\n",
            "Epoch 385/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1107 - accuracy: 0.1520\n",
            "Epoch 386/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1111 - accuracy: 0.1519\n",
            "Epoch 387/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1110 - accuracy: 0.1519\n",
            "Epoch 388/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1103 - accuracy: 0.1520\n",
            "Epoch 389/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1103 - accuracy: 0.1521\n",
            "Epoch 390/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1105 - accuracy: 0.1520\n",
            "Epoch 391/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1108 - accuracy: 0.1520\n",
            "Epoch 392/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1106 - accuracy: 0.1521\n",
            "Epoch 393/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1104 - accuracy: 0.1521\n",
            "Epoch 394/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1105 - accuracy: 0.1521\n",
            "Epoch 395/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1101 - accuracy: 0.1522\n",
            "Epoch 396/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1098 - accuracy: 0.1522\n",
            "Epoch 397/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1099 - accuracy: 0.1522\n",
            "Epoch 398/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1100 - accuracy: 0.1521\n",
            "Epoch 399/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1091 - accuracy: 0.1521\n",
            "Epoch 400/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1099 - accuracy: 0.1522\n",
            "Epoch 401/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1096 - accuracy: 0.1521\n",
            "Epoch 402/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1100 - accuracy: 0.1521\n",
            "Epoch 403/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1092 - accuracy: 0.1523\n",
            "Epoch 404/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1094 - accuracy: 0.1523\n",
            "Epoch 405/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1086 - accuracy: 0.1524\n",
            "Epoch 406/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1091 - accuracy: 0.1521\n",
            "Epoch 407/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1091 - accuracy: 0.1524\n",
            "Epoch 408/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1092 - accuracy: 0.1521\n",
            "Epoch 409/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1089 - accuracy: 0.1521\n",
            "Epoch 410/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1084 - accuracy: 0.1523\n",
            "Epoch 411/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1085 - accuracy: 0.1523\n",
            "Epoch 412/500\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1086 - accuracy: 0.1524\n",
            "Epoch 413/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1086 - accuracy: 0.1524\n",
            "Epoch 414/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1092 - accuracy: 0.1523\n",
            "Epoch 415/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1084 - accuracy: 0.1524\n",
            "Epoch 416/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1084 - accuracy: 0.1524\n",
            "Epoch 417/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1081 - accuracy: 0.1525\n",
            "Epoch 418/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1081 - accuracy: 0.1524\n",
            "Epoch 419/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1084 - accuracy: 0.1523\n",
            "Epoch 420/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1078 - accuracy: 0.1524\n",
            "Epoch 421/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1081 - accuracy: 0.1523\n",
            "Epoch 422/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1083 - accuracy: 0.1523\n",
            "Epoch 423/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1080 - accuracy: 0.1523\n",
            "Epoch 424/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1075 - accuracy: 0.1524\n",
            "Epoch 425/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1078 - accuracy: 0.1525\n",
            "Epoch 426/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1078 - accuracy: 0.1523\n",
            "Epoch 427/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1079 - accuracy: 0.1524\n",
            "Epoch 428/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1077 - accuracy: 0.1525\n",
            "Epoch 429/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1075 - accuracy: 0.1526\n",
            "Epoch 430/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1075 - accuracy: 0.1525\n",
            "Epoch 431/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1073 - accuracy: 0.1525\n",
            "Epoch 432/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1066 - accuracy: 0.1527\n",
            "Epoch 433/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1069 - accuracy: 0.1526\n",
            "Epoch 434/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1072 - accuracy: 0.1525\n",
            "Epoch 435/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1074 - accuracy: 0.1525\n",
            "Epoch 436/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1067 - accuracy: 0.1528\n",
            "Epoch 437/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1064 - accuracy: 0.1526\n",
            "Epoch 438/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1072 - accuracy: 0.1525\n",
            "Epoch 439/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1066 - accuracy: 0.1525\n",
            "Epoch 440/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1065 - accuracy: 0.1526\n",
            "Epoch 441/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1065 - accuracy: 0.1528\n",
            "Epoch 442/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1071 - accuracy: 0.1524\n",
            "Epoch 443/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1070 - accuracy: 0.1525\n",
            "Epoch 444/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1069 - accuracy: 0.1527\n",
            "Epoch 445/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1061 - accuracy: 0.1529\n",
            "Epoch 446/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1057 - accuracy: 0.1529\n",
            "Epoch 447/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1060 - accuracy: 0.1528\n",
            "Epoch 448/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1056 - accuracy: 0.1527\n",
            "Epoch 449/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1061 - accuracy: 0.1528\n",
            "Epoch 450/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1060 - accuracy: 0.1527\n",
            "Epoch 451/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1062 - accuracy: 0.1527\n",
            "Epoch 452/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1055 - accuracy: 0.1529\n",
            "Epoch 453/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1057 - accuracy: 0.1527\n",
            "Epoch 454/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1052 - accuracy: 0.1529\n",
            "Epoch 455/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1052 - accuracy: 0.1527\n",
            "Epoch 456/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1056 - accuracy: 0.1528\n",
            "Epoch 457/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1054 - accuracy: 0.1527\n",
            "Epoch 458/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1048 - accuracy: 0.1528\n",
            "Epoch 459/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1065 - accuracy: 0.1525\n",
            "Epoch 460/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1059 - accuracy: 0.1529\n",
            "Epoch 461/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1054 - accuracy: 0.1529\n",
            "Epoch 462/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1055 - accuracy: 0.1529\n",
            "Epoch 463/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1049 - accuracy: 0.1529\n",
            "Epoch 464/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1054 - accuracy: 0.1528\n",
            "Epoch 465/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1048 - accuracy: 0.1531\n",
            "Epoch 466/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1048 - accuracy: 0.1529\n",
            "Epoch 467/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1047 - accuracy: 0.1528\n",
            "Epoch 468/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1046 - accuracy: 0.1529\n",
            "Epoch 469/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1045 - accuracy: 0.1530\n",
            "Epoch 470/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1047 - accuracy: 0.1529\n",
            "Epoch 471/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1042 - accuracy: 0.1530\n",
            "Epoch 472/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1048 - accuracy: 0.1529\n",
            "Epoch 473/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1052 - accuracy: 0.1528\n",
            "Epoch 474/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1046 - accuracy: 0.1529\n",
            "Epoch 475/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1036 - accuracy: 0.1531\n",
            "Epoch 476/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1037 - accuracy: 0.1530\n",
            "Epoch 477/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1042 - accuracy: 0.1529\n",
            "Epoch 478/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1042 - accuracy: 0.1530\n",
            "Epoch 479/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1046 - accuracy: 0.1528\n",
            "Epoch 480/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1043 - accuracy: 0.1528\n",
            "Epoch 481/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1039 - accuracy: 0.1531\n",
            "Epoch 482/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1044 - accuracy: 0.1530\n",
            "Epoch 483/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1037 - accuracy: 0.1532\n",
            "Epoch 484/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1043 - accuracy: 0.1529\n",
            "Epoch 485/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1039 - accuracy: 0.1531\n",
            "Epoch 486/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1044 - accuracy: 0.1530\n",
            "Epoch 487/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1039 - accuracy: 0.1532\n",
            "Epoch 488/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1034 - accuracy: 0.1532\n",
            "Epoch 489/500\n",
            "370/370 [==============================] - 24s 64ms/step - loss: 0.1030 - accuracy: 0.1532\n",
            "Epoch 490/500\n",
            "370/370 [==============================] - 24s 64ms/step - loss: 0.1038 - accuracy: 0.1531\n",
            "Epoch 491/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1032 - accuracy: 0.1532\n",
            "Epoch 492/500\n",
            "370/370 [==============================] - 24s 64ms/step - loss: 0.1031 - accuracy: 0.1533\n",
            "Epoch 493/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1034 - accuracy: 0.1531\n",
            "Epoch 494/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1033 - accuracy: 0.1532\n",
            "Epoch 495/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1032 - accuracy: 0.1532\n",
            "Epoch 496/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1034 - accuracy: 0.1531\n",
            "Epoch 497/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1031 - accuracy: 0.1531\n",
            "Epoch 498/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1034 - accuracy: 0.1532\n",
            "Epoch 499/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1033 - accuracy: 0.1531\n",
            "Epoch 500/500\n",
            "370/370 [==============================] - 23s 63ms/step - loss: 0.1027 - accuracy: 0.1533\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1e1e2503d0>"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_answers(input_question_list)"
      ],
      "metadata": {
        "id": "ABgmYQKJMApo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e42b648c-f7cc-48ff-ee4d-1042122ff2e2"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력 : 안녕 오랜만이야\n",
            "출력 : 많이 힘들지 않았으면 좋겠네요 .\n",
            "\n",
            "입력 : 잘 지냈어?\n",
            "출력 : 안부를 물어주시다니 감사합니다 .\n",
            "\n",
            "입력 : 뭐 먹을까\n",
            "출력 : 노래 연습을 해보세요 .\n",
            "\n",
            "입력 : 메뉴 추천해줘\n",
            "출력 : 냉장고 파먹기 해보세요 .\n",
            "\n",
            "입력 : 어제 뭐 먹었어?\n",
            "출력 : 연락해보세요 .\n",
            "\n",
            "입력 : 나랑 놀자\n",
            "출력 : 저랑 놀아요 .\n",
            "\n",
            "입력 : 어디 가고 싶어?\n",
            "출력 : 저 주세요 .\n",
            "\n",
            "입력 : 널 만나서 기뻐\n",
            "출력 : 더 잔소리해보세요 .\n",
            "\n",
            "입력 : 웃어줄래\n",
            "출력 : 뇌세포에 에너지를 공급하려는 자연스러운 현상이에요 . 에너지가 부족한가봐요 .\n",
            "\n",
            "입력 : 삶은 뭘까?\n",
            "출력 : 저는 있다고 믿어요 .\n",
            "\n",
            "입력 : 인생살이 왜이리 힘드냐\n",
            "출력 : 얼른 만나러가세요 .\n",
            "\n",
            "입력 : 이제 그만 쉬고 싶어\n",
            "출력 : 아무 생각 하지 말고 쉬세요 .\n",
            "\n",
            "입력 : 너무 고독하다\n",
            "출력 : 생각만해도 달콤하네요 .\n",
            "\n",
            "입력 : 죽으면 어떻게 될까\n",
            "출력 : 인생은 반복의 연속이지요 .\n",
            "\n",
            "입력 : 내가 죽으면 슬퍼해 줄거야?\n",
            "출력 : 당신이 좋아하는 꽃으로 만들어보세요 .\n",
            "\n",
            "입력 : 도망가고 싶다\n",
            "출력 : 시간은 상대적으로 흘러갑니다 .\n",
            "\n",
            "입력 : 우리 마지막이야\n",
            "출력 : 궁금할 수 있어요 .\n",
            "\n",
            "입력 : 나 간다 잘 지내 안녕\n",
            "출력 : 실감이 안날 거예요 .\n",
            "\n",
            "입력 : 넌 최고였어\n",
            "출력 : 용기내서 잡아보세요 .\n",
            "\n",
            "입력 : 나 잊지마\n",
            "출력 : 하고 싶은 말 다하세요 .\n",
            "\n",
            "입력 : 나는 심장이 없어\n",
            "출력 : 그 사람도 그럴 거예요 .\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#결론\n",
        "\n",
        "이번 프로젝트에서는 한국어 데이터를 이용하여 챗봇을 만들어보았습니다.\n",
        "\n",
        "토크나이저는 SubwordTextEncoder를 이용하였고, 트랜스포머 모델을 이용하여 만들어 보았습니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "FCTEKNnkMVj1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 회고\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "## - 이번 프로젝트에서 어려웠던 점\n",
        "코랩에서 학습을 진행하다 보니 에포크를 높여서 진행할 때 학습시간이 오래걸려 결과를 비교하는데 시간이 오래 걸렸던게 좀 어려웠습니다.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## - 프로젝트를 진행하면서 알게된 점\n",
        "인코더와 디코더의 개념을 다시 상기시켰고 멀티헤드 어텐션이 무엇인지 알게되었습니다.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## - 루브릭 평가지표를 맞추기 위해 시도한 것들\n",
        "에포크 수를 증가시켜보았고, 질문을 좀 다양하게 넣어 학습시켜보았습니다.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## - 자기다짐\n",
        "밥벌어 먹고 살려면 공부를 열심히하자! \n",
        "\n"
      ],
      "metadata": {
        "id": "w8pEXbMHMWd5"
      }
    }
  ]
}