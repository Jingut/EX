{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exp_6.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1EAduRg12hlpQlyr7TlQDXaL7sfuQcv1G",
      "authorship_tag": "ABX9TyPnrlX1ybc0sCMRBsP2b3YV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jingut/EX/blob/main/Exp_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터 읽어오기"
      ],
      "metadata": {
        "id": "r8oM_aIEgDC0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import glob\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "txt_file_path = '/content/drive/MyDrive/lyrics/*'\n",
        "\n",
        "txt_list = glob.glob(txt_file_path)\n",
        "\n",
        "raw_corpus = []\n",
        "\n",
        "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
        "for txt_file in txt_list:\n",
        "    with open(txt_file, \"r\") as f:\n",
        "        raw = f.read().splitlines()\n",
        "        raw_corpus.extend(raw)        \n",
        "\n",
        "print(\"데이터 크기:\", len(raw_corpus))\n",
        "print(\"Examples:\\n\", raw_corpus[:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgnDW3xjGLnf",
        "outputId": "7b99f3db-1c33-4cf5-bd8d-0736cc56e9ae"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "데이터 크기: 227088\n",
            "Examples:\n",
            " ['Looking for some education', 'Made my way into the night', 'All that bullshit conversation']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(raw_corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDvxGOxDhZ7k",
        "outputId": "f95f9148-337d-4cae-f35e-7534b3fae29c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "227088"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터 정제"
      ],
      "metadata": {
        "id": "FvImbYG8gGzy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, sentence in enumerate(raw_corpus):\n",
        "  if len(sentence) == 0: continue   # 길이가 0인 문장은 건너뜁니다.\n",
        "  if idx > 9: break   # 일단 문장 10개만 확인해 볼 겁니다.\n",
        "      \n",
        "  print(sentence)"
      ],
      "metadata": {
        "id": "RJ6sSv-HIwxf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddf842a2-56b0-42a8-c6f3-46e4771c9cfd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking for some education\n",
            "Made my way into the night\n",
            "All that bullshit conversation\n",
            "Baby, can't you read the signs? I won't bore you with the details, baby\n",
            "I don't even wanna waste your time\n",
            "Let's just say that maybe\n",
            "You could help me ease my mind\n",
            "I ain't Mr. Right But if you're looking for fast love\n",
            "If that's love in your eyes\n",
            "It's more than enough\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_sentence(sentence):\n",
        "  sentence = sentence.lower().strip() \n",
        "  sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) \n",
        "  sentence = re.sub(r'[\" \"]+', \" \", sentence) \n",
        "  sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) \n",
        "  sentence = sentence.strip() \n",
        "  sentence = '<start> ' + sentence + ' <end>' \n",
        "  return sentence"
      ],
      "metadata": {
        "id": "Tsy6cuKGgSPf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = []\n",
        "\n",
        "for sentence in raw_corpus:\n",
        "  # 우리가 원하지 않는 문장은 건너뜁니다\n",
        "  if len(sentence) == 0: continue\n",
        "  if sentence[-1] == \":\": continue\n",
        "  # 정제를 하고 담아주세요\n",
        "  preprocessed_sentence = preprocess_sentence(sentence)\n",
        "  corpus.append(preprocessed_sentence)"
      ],
      "metadata": {
        "id": "THGvZAOOkMcu"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(corpus):\n",
        "  tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "  num_words=12000, \n",
        "  filters=' ',\n",
        "  oov_token=\"<unk>\")\n",
        "  # corpus를 이용해 tokenizer 내부의 단어장을 완성합니다\n",
        "  tokenizer.fit_on_texts(corpus)\n",
        "  # 준비한 tokenizer를 이용해 corpus를 Tensor로 변환합니다\n",
        "  tensor = tokenizer.texts_to_sequences(corpus)   \n",
        "  # 입력 데이터의 시퀀스 길이를 일정하게 맞춰줍니다\n",
        "  # 만약 시퀀스가 짧다면 문장 뒤에 패딩을 붙여 길이를 맞춰줍니다.\n",
        "  # 문장 앞에 패딩을 붙여 길이를 맞추고 싶다면 padding='pre'를 사용합니다\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post',maxlen=21)#21로 길이제한\n",
        "  \n",
        "  print(tensor,tokenizer)\n",
        "  return tensor, tokenizer\n",
        "\n",
        "tensor, tokenizer = tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VQs4RoMkSjo",
        "outputId": "170927a8-64df-4ceb-ea36-82b8a143660a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[   2  339   26 ...    0    0    0]\n",
            " [   2  202   13 ...    0    0    0]\n",
            " [   2   25   16 ...    0    0    0]\n",
            " ...\n",
            " [   2  848    1 ...    0    0    0]\n",
            " [   2  161   64 ...    0    0    0]\n",
            " [   2 4396  161 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f88c478f650>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJUwfYxY51XH",
        "outputId": "98291541-4b46-41b7-f3db-d0cb8e56f494"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "199764"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tensor[5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kreVyIe65iD6",
        "outputId": "06a76714-6b07-4b6d-ca1d-d511f8a1efa1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다\n",
        "# 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높습니다.\n",
        "src_input = tensor[:, :-1]  \n",
        "# tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\n",
        "tgt_input = tensor[:, 1:]    \n",
        "\n",
        "print(src_input[0])\n",
        "print(tgt_input[0])\n",
        "# 패딩을 앞에둘지 뒤에둘지에 따라 성능이 달라짐"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXWKuenF6Vhm",
        "outputId": "e89a6654-87f7-48e5-850e-173aafcd339c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[   2  339   26  100 5027    3    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0]\n",
            "[ 339   26  100 5027    3    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 평가 데이터셋 분리"
      ],
      "metadata": {
        "id": "u0Rb-xx9gJV6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = len(src_input)\n",
        "BATCH_SIZE = 256\n",
        "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
        "\n",
        " # tokenizer가 구축한 단어사전 내 7000개와, 여기 포함되지 않은 0:<pad>를 포함하여 7001개\n",
        "VOCAB_SIZE = tokenizer.num_words + 1   \n",
        "\n",
        "# 준비한 데이터 소스로부터 데이터셋을 만듭니다\n",
        "# 데이터셋에 대해서는 아래 문서를 참고하세요\n",
        "# 자세히 알아둘수록 도움이 많이 되는 중요한 문서입니다\n",
        "# https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
        "\n",
        "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, random_state=20, shuffle = True, test_size=0.2)\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
        "print(dataset)\n",
        "dataset = dataset.shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbKuUqBnktpU",
        "outputId": "50dc5633-eb11-4064-e366-e4d71052de1c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<TensorSliceDataset element_spec=(TensorSpec(shape=(20,), dtype=tf.int32, name=None), TensorSpec(shape=(20,), dtype=tf.int32, name=None))>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset element_spec=(TensorSpec(shape=(256, 20), dtype=tf.int32, name=None), TensorSpec(shape=(256, 20), dtype=tf.int32, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(src_input[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTXD0RJCL5HU",
        "outputId": "5ef509f5-ab16-45d2-f9a1-a7f37c72899e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 인공지능 만들기\n"
      ],
      "metadata": {
        "id": "_dumGasFgQaL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
        "    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환합니다\n",
        "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
        "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
        "    end_token = tokenizer.word_index[\"<end>\"]\n",
        "\n",
        "    # 단어 하나씩 예측해 문장을 만듭니다\n",
        "    #    1. 입력받은 문장의 텐서를 입력합니다\n",
        "    #    2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n",
        "    #    3. 2에서 예측된 word index를 문장 뒤에 붙입니다\n",
        "    #    4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다\n",
        "    while True:\n",
        "        # 1\n",
        "        predict = model(test_tensor) \n",
        "        # 2\n",
        "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
        "        # 3 \n",
        "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
        "        # 4\n",
        "        if predict_word.numpy()[0] == end_token: break\n",
        "        if test_tensor.shape[1] >= max_len: break\n",
        "\n",
        "    generated = \"\"\n",
        "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n",
        "    for word_index in test_tensor[0].numpy():\n",
        "        generated += tokenizer.index_word[word_index] + \" \"\n",
        "\n",
        "    return generated"
      ],
      "metadata": {
        "id": "d2DHQg1U-tnz"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextGenerator(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_size, hidden_size):\n",
        "      super().__init__()\n",
        "      \n",
        "      self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
        "      self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
        "      self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
        "      self.linear = tf.keras.layers.Dense(vocab_size)\n",
        "      \n",
        "  def call(self, x):\n",
        "      out = self.embedding(x)\n",
        "      out = self.rnn_1(out)\n",
        "      out = self.rnn_2(out)\n",
        "      out = self.linear(out)\n",
        "      \n",
        "      return out\n",
        "    \n",
        "embedding_size = 256\n",
        "hidden_size = 1024\n",
        "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
      ],
      "metadata": {
        "id": "7nEEWU9Igb6d"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋에서 데이터 한 배치만 불러오는 방법입니다.\n",
        "# 지금은 동작 원리에 너무 빠져들지 마세요~\n",
        "for src_sample, tgt_sample in dataset.take(1): break\n",
        "\n",
        "# 한 배치만 불러온 데이터를 모델에 넣어봅니다\n",
        "model(src_sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrQV8USE98n3",
        "outputId": "6435a469-f3e0-4f3c-9128-6cad4109f713"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(256, 20, 12001), dtype=float32, numpy=\n",
              "array([[[ 5.61067827e-05, -2.35063751e-04,  1.03112237e-04, ...,\n",
              "          9.20737293e-05, -9.88590691e-05,  9.26610446e-05],\n",
              "        [ 5.06080396e-05, -2.82036082e-04,  4.02501100e-05, ...,\n",
              "          1.83709810e-04, -1.45609665e-04, -3.44066848e-06],\n",
              "        [ 2.85499700e-04, -3.46019689e-04,  6.62462844e-05, ...,\n",
              "          3.33856908e-04, -2.27887373e-04, -2.15124703e-04],\n",
              "        ...,\n",
              "        [ 3.42460210e-03,  4.60605137e-04, -2.48633535e-03, ...,\n",
              "         -9.51610506e-04,  6.26751687e-04,  1.89533865e-03],\n",
              "        [ 3.86591791e-03,  6.18351041e-04, -2.83506117e-03, ...,\n",
              "         -1.04904699e-03,  6.65441388e-04,  2.11543194e-03],\n",
              "        [ 4.27390961e-03,  7.42815610e-04, -3.10622714e-03, ...,\n",
              "         -1.13556674e-03,  7.11450179e-04,  2.28946772e-03]],\n",
              "\n",
              "       [[ 5.61067827e-05, -2.35063751e-04,  1.03112237e-04, ...,\n",
              "          9.20737293e-05, -9.88590691e-05,  9.26610446e-05],\n",
              "        [ 8.43302332e-05, -3.03600100e-04,  1.62402401e-04, ...,\n",
              "          1.95918692e-04,  6.00500098e-05,  1.30395201e-04],\n",
              "        [-1.09042478e-04, -2.90276250e-04,  9.50184185e-05, ...,\n",
              "          2.51596561e-04,  1.97765068e-04,  4.25202714e-04],\n",
              "        ...,\n",
              "        [ 3.87456431e-03,  5.61918016e-04, -3.57524073e-03, ...,\n",
              "         -9.07912559e-04,  6.70497946e-04,  2.59678042e-03],\n",
              "        [ 4.29240987e-03,  6.30450784e-04, -3.68519756e-03, ...,\n",
              "         -1.01820659e-03,  7.27647042e-04,  2.66935257e-03],\n",
              "        [ 4.67167515e-03,  6.79891091e-04, -3.75486026e-03, ...,\n",
              "         -1.12131319e-03,  7.91605504e-04,  2.72133364e-03]],\n",
              "\n",
              "       [[ 5.61067827e-05, -2.35063751e-04,  1.03112237e-04, ...,\n",
              "          9.20737293e-05, -9.88590691e-05,  9.26610446e-05],\n",
              "        [ 1.27017498e-04, -4.78598988e-04,  1.14169634e-04, ...,\n",
              "          5.32255726e-05,  5.51145931e-05,  5.75946724e-05],\n",
              "        [ 3.19338069e-05, -7.18293071e-04,  1.93124215e-04, ...,\n",
              "          1.19423028e-04, -5.62468776e-05,  1.22521844e-04],\n",
              "        ...,\n",
              "        [ 2.97335815e-03,  1.03625550e-03, -3.05103278e-03, ...,\n",
              "         -3.26692243e-04,  3.63058905e-04,  1.66486169e-03],\n",
              "        [ 3.50826862e-03,  1.10630109e-03, -3.27826478e-03, ...,\n",
              "         -4.54854773e-04,  4.35368536e-04,  1.90408155e-03],\n",
              "        [ 3.99716338e-03,  1.15118595e-03, -3.44329141e-03, ...,\n",
              "         -5.79880143e-04,  5.11749939e-04,  2.10152543e-03]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[ 5.61067827e-05, -2.35063751e-04,  1.03112237e-04, ...,\n",
              "          9.20737293e-05, -9.88590691e-05,  9.26610446e-05],\n",
              "        [-6.69049332e-05, -3.39682447e-04,  1.16337171e-04, ...,\n",
              "          2.01506875e-04,  5.84331028e-05, -3.55037719e-06],\n",
              "        [-2.99315958e-04, -3.50837014e-04, -7.31799882e-05, ...,\n",
              "          2.36149295e-04,  3.39398321e-05, -1.13933493e-04],\n",
              "        ...,\n",
              "        [ 1.94579212e-03,  7.64772762e-04, -1.79726828e-03, ...,\n",
              "         -1.56971940e-03,  1.39716445e-04,  1.62109791e-03],\n",
              "        [ 2.48420658e-03,  9.40902741e-04, -2.25083972e-03, ...,\n",
              "         -1.50779681e-03,  1.83307697e-04,  1.90863037e-03],\n",
              "        [ 2.99484446e-03,  1.07819808e-03, -2.63675419e-03, ...,\n",
              "         -1.45303400e-03,  2.41674308e-04,  2.14955141e-03]],\n",
              "\n",
              "       [[ 5.61067827e-05, -2.35063751e-04,  1.03112237e-04, ...,\n",
              "          9.20737293e-05, -9.88590691e-05,  9.26610446e-05],\n",
              "        [ 1.96149020e-04, -3.82464350e-04,  1.78928327e-04, ...,\n",
              "          2.29287471e-04, -1.57387076e-05,  3.82304803e-04],\n",
              "        [ 1.65934092e-04, -1.56284852e-06,  1.10695095e-04, ...,\n",
              "          3.71784932e-04, -3.24785156e-04,  5.09785255e-04],\n",
              "        ...,\n",
              "        [-9.22007544e-04, -6.41559600e-04,  1.56505484e-04, ...,\n",
              "         -4.51661093e-04, -4.26294078e-04, -4.87258221e-04],\n",
              "        [-8.61454755e-04, -5.06302749e-04,  4.53760818e-04, ...,\n",
              "         -6.63711049e-04, -5.66339819e-04, -5.53149206e-04],\n",
              "        [-5.63971233e-04, -2.97746476e-04,  3.57274374e-04, ...,\n",
              "         -8.24425893e-04, -6.36727316e-04, -3.81133170e-04]],\n",
              "\n",
              "       [[ 5.61067827e-05, -2.35063751e-04,  1.03112237e-04, ...,\n",
              "          9.20737293e-05, -9.88590691e-05,  9.26610446e-05],\n",
              "        [ 1.64738536e-04, -3.74713738e-04,  6.59153011e-05, ...,\n",
              "         -5.80994711e-05, -1.35791968e-04,  9.62004488e-05],\n",
              "        [ 1.68609331e-04, -7.06511375e-04, -5.35698964e-05, ...,\n",
              "         -2.13087449e-04,  2.82941997e-04,  3.93913469e-05],\n",
              "        ...,\n",
              "        [ 1.03216837e-04, -1.82387608e-04,  2.12624494e-04, ...,\n",
              "          2.34561467e-05,  5.50267323e-05,  3.47324676e-04],\n",
              "        [ 6.63425075e-04,  9.19145386e-05, -4.30233282e-04, ...,\n",
              "         -2.97771112e-05,  4.19116441e-05,  8.39895045e-04],\n",
              "        [ 1.23851537e-03,  3.38874117e-04, -1.09276420e-03, ...,\n",
              "         -9.57976299e-05,  6.31633520e-05,  1.28538697e-03]]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "Oewonp48gfi4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec3cd4a8-655c-43e2-f3cb-41aeb852a06e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"text_generator\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  3072256   \n",
            "                                                                 \n",
            " lstm (LSTM)                 multiple                  5246976   \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               multiple                  8392704   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  12301025  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 29,012,961\n",
            "Trainable params: 29,012,961\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# optimizer와 loss등은 차차 배웁니다 \n",
        "# 혹시 미리 알고 싶다면 아래 문서를 참고하세요\n",
        "\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/losses\n",
        "# 양이 상당히 많은 편이니 지금 보는 것은 추천하지 않습니다\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True,\n",
        "    reduction='none'\n",
        ")\n",
        "\n",
        "model.compile(loss=loss, optimizer=optimizer)\n",
        "\n",
        "# model.fit() 함수에 들어가는 다양한 인자를 알고 싶다면 아래의 문서를 참고하세요. \n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit\n",
        "\n",
        "\n",
        "LSTM = model.fit(enc_train, dec_train, epochs=10, validation_data=(enc_val, dec_val))"
      ],
      "metadata": {
        "id": "PqL_NOSGghlY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74b5490a-a00c-46ef-bf8a-37848c4b1395"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "4995/4995 [==============================] - 162s 32ms/step - loss: 2.4694 - val_loss: 2.2791\n",
            "Epoch 2/10\n",
            "4995/4995 [==============================] - 158s 32ms/step - loss: 2.1660 - val_loss: 2.1471\n",
            "Epoch 3/10\n",
            "4995/4995 [==============================] - 159s 32ms/step - loss: 1.9964 - val_loss: 2.0741\n",
            "Epoch 4/10\n",
            "4995/4995 [==============================] - 158s 32ms/step - loss: 1.8489 - val_loss: 2.0305\n",
            "Epoch 5/10\n",
            "4995/4995 [==============================] - 158s 32ms/step - loss: 1.7133 - val_loss: 2.0113\n",
            "Epoch 6/10\n",
            "4995/4995 [==============================] - 159s 32ms/step - loss: 1.5887 - val_loss: 2.0019\n",
            "Epoch 7/10\n",
            "4995/4995 [==============================] - 158s 32ms/step - loss: 1.4760 - val_loss: 2.0009\n",
            "Epoch 8/10\n",
            "4995/4995 [==============================] - 159s 32ms/step - loss: 1.3748 - val_loss: 2.0093\n",
            "Epoch 9/10\n",
            "4995/4995 [==============================] - 158s 32ms/step - loss: 1.2845 - val_loss: 2.0298\n",
            "Epoch 10/10\n",
            "4995/4995 [==============================] - 158s 32ms/step - loss: 1.2039 - val_loss: 2.0578\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 인공지능 평가\n"
      ],
      "metadata": {
        "id": "UcbHTRD-JInF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Loss\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')"
      ],
      "metadata": {
        "id": "aRTVEy51Dob5"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''LSTM = model.fit(enc_train, dec_train, epochs=10, shuffle = True, validation_data=(enc_val, dec_val))\n",
        "\n",
        "# 진행도 확인\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(LSTM .history['loss'])\n",
        "plt.plot(LSTM .history['val_loss'])\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['train_loss', 'val_loss'])\n",
        "plt.show()'''"
      ],
      "metadata": {
        "id": "3wYFyVOUxF9p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "cfa5d5fd-a501-4f9a-a47c-27b102eb5d28"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"LSTM = model.fit(enc_train, dec_train, epochs=10, shuffle = True, validation_data=(enc_val, dec_val))\\n\\n# 진행도 확인\\nimport matplotlib.pyplot as plt\\nplt.plot(LSTM .history['loss'])\\nplt.plot(LSTM .history['val_loss'])\\nplt.ylabel('Loss')\\nplt.xlabel('Epoch')\\nplt.legend(['train_loss', 'val_loss'])\\nplt.show()\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
      ],
      "metadata": {
        "id": "0enHBFs7GzJ6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "7879486d-c32e-4888-8a03-89da1cca81c6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<start> i love you so much , i love you so much , i love '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  결론\n",
        "이번 프로젝트에서는 RNN으로 여러 가사들을 학습시켜 인공지능 작사를 하는 모델을 학습시켜 보았습니다.\n",
        "\n",
        "\n",
        "데이터 전처리 과정이 매우 까다로웠는데 정규표현식을 이용하여 학습에 필요없는 특수문자나 공백을 제거하고 문장 토큰화를 거쳐 RNN모델에 통과시켜 특수한 단어나 문장을 지정해주었을 때 학습된 모델이 어떻게 작사를 해내는지까지 구현해보았습니다.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3ZmHyPjB-NeA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 회고\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "## - 이번 프로젝트에서 어려웠던 점\n",
        "\n",
        "* 기본적인 전처리는 어느정도 되어 있어 간단하였지만 루브릭지표를 맞추는데 꽤나 시간이 걸렸고 학습시간 또한 만만치 않아서 애를 먹었습니다.\n",
        "\n",
        "> 그리고 추천해주신 토큰의 길이가 15였는데 직접 그렇게 설정해보니 하이퍼파라미터를 어떻게 설정하여도 val_loss가 2.4 밑으로 내려가지 않아 힘들었는데 max_len의 길이를 조금 더 늘였더니 val_loss가 눈에 띄게 떨어져서 해결이 되는 부분이 있었습니다.\n",
        "\n",
        "> 또한 주어진 조건인 10 에포크 제한이 있어 힘들었던 것 같습니다. 조금만 더 돌렸다면 val_loss가 2.2 이하로 떨어졌을 수 있지 않을까라는 생각도 해보았습니다.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "## - 루브릭 평가지표를 맞추기 위해 시도한 것들\n",
        "\n",
        "* max_len을 15로 지정하였을 때 제가 시도해본 것들은 일단 rnn의 레이어 수를 최대 6층까지 \n",
        "확장 해보았었습니다.\n",
        "\n",
        "\n",
        "* 모델의 Embedding Size와 Hidden Size를 2배수 단위로 늘여서 진행해보았습니다.\n",
        "\n",
        "\n",
        "* 학습시간을 줄여보려 배치사이즈 또한 2배수로 늘려가면서 진행해보았습니다.\n",
        "\n",
        "\n",
        "* padding을 pre, post 로 나눠 진행해보았습니다.\n",
        "\n",
        "\n",
        "* 그리고 주어진 파일에 동일인의 파일인데 -,_ 차이로 파일의 문장순서만 다른 것이 있어서 중복 텍스트 파일을 삭제하고도 진행해보았습니다.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## - 프로젝트를 진행하면서 알게된 점\n",
        "\n",
        "* max_len을 어떻게 설정해주느냐에 따라 모델이 이해하는 문맥의 수준이 올라간다는 것을 val_loss를 통해 알수 있었습니다.\n",
        "\n",
        "\n",
        "* 모델의 Embedding Size와 Hidden Size를 늘려 보았지만 학습시간도 늘고 val_loss도 늘었습니다...그냥 무작정 늘리기만하면 능사가 아니라는 것을 깨달았습니다.\n",
        "\n",
        "\n",
        "* 이번 프로젝트에서 배치사이즈를 늘린다고 학습시간이 눈에 띄게 줄어들지 않는다는 것을 알았습니다.\n",
        "\n",
        "\n",
        "* 제가 생각하기엔 RNN에서는 뒤편에 padding을 넣는 것 보다는 앞쪽에 padding을 넣어주는 편이 마지막 결과에 paddind이 미치는 영향이 적어져서 더 좋은 성능을 낼 것이라고 생각했었는데 \n",
        "실제로 돌려보니 post로 지정해주는 것이 로스값이 적게 나와서 패딩을 post로 두고 진행하였습니다.  \n",
        "\n",
        "\n",
        "* 텍스트파일의 중복을 제거해주면 조금 더 나아질 것 같아 진행해 보았지만 val_loss 값의 별 차이는 없었습니다.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## - 자기다짐\n",
        "\n",
        "* cv를 열심히 해야겠다고 생각했습니다.\n",
        "\n",
        "* 자연어처리 전처리가 쉽지 않다는 것을 느꼈고 정규표현식, 패딩에 이해와 관련된 공부가 필요하다고 생각했습니다.\n",
        " \n",
        "\n"
      ],
      "metadata": {
        "id": "6MTDE-Pp-Qz6"
      }
    }
  ]
}